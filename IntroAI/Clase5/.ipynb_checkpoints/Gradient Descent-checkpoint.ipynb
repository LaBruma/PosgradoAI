{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresiones + Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "# Cargo el módulo de numpy\n",
    "#-------------------------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "#Si queremos que las imágenes sean mostradas en una ventana emergente quitar el inline\n",
    "%matplotlib  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de las clases\n",
    "#=========================\n",
    "\n",
    "# Definición de la clase para levantar (y dividir) los datos\n",
    "#===========================================================\n",
    "class Data(object):\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.dataset = self._build_dataset(path)\n",
    "\n",
    "    def _build_dataset(self, path):\n",
    "        # Armo una estructura de datos para guardarlos ahí\n",
    "        #-------------------------------------------------\n",
    "        structure = [('income', np.float),\n",
    "                     ('happiness', np.float)]\n",
    "        \n",
    "        # Abro el archivo lo recorro llenando la estructura creada línea a línea\n",
    "        #-----------------------------------------------------------------------\n",
    "        with open(path, encoding=\"utf8\") as data_csv:\n",
    "\n",
    "            data_gen = ((float(line.split(',')[1]), float(line.split(',')[2])) # add here + 10 in second value\n",
    "                        for i, line in enumerate(data_csv) if i != 0)\n",
    "            embeddings = np.fromiter(data_gen, structure)\n",
    "\n",
    "        return embeddings\n",
    "    \n",
    "    # Separo los los datos (train y test)\n",
    "    #------------------------------------\n",
    "    def split(self, percentage): # 0.8\n",
    "        X = self.dataset['income']\n",
    "        y = self.dataset['happiness']\n",
    "\n",
    "        permuted_idxs = np.random.permutation(X.shape[0])\n",
    "\n",
    "        train_idxs = permuted_idxs[0:int(percentage * X.shape[0])]\n",
    "\n",
    "        test_idxs = permuted_idxs[int(percentage * X.shape[0]): X.shape[0]]\n",
    "\n",
    "        X_train = X[train_idxs]\n",
    "        X_test = X[test_idxs]\n",
    "\n",
    "        y_train = y[train_idxs]\n",
    "        y_test = y[test_idxs]\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Clase base de la que heredan las que vayamos implementando\n",
    "#-----------------------------------------------------------\n",
    "# Es conveniente tener una clase base de la que vayan heredando las demás. Siempre habrá un método fit\n",
    "# y un método predict. Pero en esta clase base puede haber definiciones de atributos comunes a todas\n",
    "#===========================================================\n",
    "class BaseModel(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        return NotImplemented\n",
    "\n",
    "    def predict(self, X):\n",
    "        return NotImplemented\n",
    "\n",
    "\n",
    "class ConstantModel(BaseModel):\n",
    "    # El modelo constante solo saca la media de los datos y devuelve ese valor\n",
    "    # Es útil para comparar. Ningún modelo debería ser peor que este.\n",
    "    #-------------------------------------------------------------------------\n",
    "    def fit(self, X, Y):\n",
    "        W = Y.mean()\n",
    "        self.model = W\n",
    "\n",
    "    def predict(self, X):\n",
    "        # La \"predicción\" consiste en devolver la media para todos los valores\n",
    "        return np.ones(len(X)) * self.model\n",
    "\n",
    "# Modelo de la regresión lineal\n",
    "#==============================\n",
    "class LinearRegression(BaseModel):\n",
    "    # Este modelo de regresión lineal ajusta únicamente la pendiente, no contempla la ordenada al origen\n",
    "    def fit(self, X, y):\n",
    "        # Verificamos si X es un vector o una matriz\n",
    "        if len(X.shape) == 1:\n",
    "            # Esta es una manera de escribir la pseudo-inversa (X'.X)^(-1).X'.y\n",
    "            W = X.T.dot(y) / X.T.dot(X)\n",
    "        else:\n",
    "            # Y esta es la manera con matrices\n",
    "            W = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "        self.model = W\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model * X\n",
    "    \n",
    "# Modelo que incluye la ordenada al origen (b)\n",
    "# ============================================\n",
    "class LinearRegressionWithB(BaseModel):\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # En el caso de ajustar con ordenada al origen le agregamos la columna de b con unos\n",
    "        # (Le agrega la fila abajo y luego traspongo --> Vectores columna)\n",
    "        X_expanded = np.vstack((X, np.ones(len(X)))).T\n",
    "        W = np.linalg.inv(X_expanded.T.dot(X_expanded)).dot(X_expanded.T).dot(y)\n",
    "        self.model = W\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_expanded = np.vstack((X, np.ones(len(X)))).T\n",
    "        return X_expanded.dot(self.model)\n",
    "\n",
    "# Modelo de la regresión cuadrática\n",
    "#==================================\n",
    "class QuadraticRegression(BaseModel):\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Armamos la matriz de ajuste\n",
    "        X_expanded = np.vstack((X**2, X, np.ones(len(X)))).T\n",
    "        W = np.linalg.inv(X_expanded.T.dot(X_expanded)).dot(X_expanded.T).dot(y)\n",
    "        \n",
    "        self.model = W\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_expanded = np.vstack((X**2, X, np.ones(len(X)))).T\n",
    "        return X_expanded.dot(self.model)\n",
    "\n",
    "# Modelo de la regresión cuadrática\n",
    "#==================================\n",
    "class PolyRegression(BaseModel):\n",
    "    \n",
    "    def fit(self, X, y, n):\n",
    "        # Tomo X y le agrego el término independiente\n",
    "        X_expanded = np.vstack((X, np.ones(len(X))))\n",
    "        for i in range(n-1):\n",
    "            # Armamos la matriz de ajuste a partir del grado del polinomio\n",
    "            X_n = X**(n-i)\n",
    "            X_expanded = np.vstack((X_n, X_expanded))\n",
    "         \n",
    "        X_expanded = X_expanded.T\n",
    "            \n",
    "        W = np.linalg.inv(X_expanded.T.dot(X_expanded)).dot(X_expanded.T).dot(y)\n",
    "        \n",
    "        self.model = W\n",
    "\n",
    "    def predict(self, X, n):\n",
    "        # Tomo X y le agrego el término independiente\n",
    "        X_expanded = np.vstack((X, np.ones(len(X))))\n",
    "        for i in range(n-1):\n",
    "            # Armamos la matriz de ajuste a partir del grado del polinomio\n",
    "            X_n = X**(n-i)\n",
    "            X_expanded = np.vstack((X_n, X_expanded))\n",
    "         \n",
    "        X_expanded = X_expanded.T\n",
    "        \n",
    "        return X_expanded.dot(self.model)\n",
    "    \n",
    "\"\"\" class GradientDescent(BaseModel)\n",
    "\n",
    "\n",
    "            \n",
    "\"\"\"\n",
    "    \n",
    "# Clases de métricas\n",
    "#===================\n",
    "class Metric(object):\n",
    "    def __call__(self, target, prediction):\n",
    "        return NotImplemented\n",
    "\n",
    "# Por ahora solo esta --> Error cuadrático medio\n",
    "class MSE(Metric):\n",
    "    def __call__(self, target, prediction):\n",
    "        n = target.size\n",
    "        return np.sum((target - prediction) ** 2) / n\n",
    "    \n",
    "\n",
    "def k_folds(X_train, y_train, k=5):\n",
    "    l_regression = LinearRegression()\n",
    "    error = MSE()\n",
    "\n",
    "    chunk_size = int(len(X_train) / k)\n",
    "    mse_list = []\n",
    "    for i in range(0, len(X_train), chunk_size):\n",
    "        end = i + chunk_size if i + chunk_size <= len(X_train) else len(X_train)\n",
    "        new_X_valid = X_train[i: end]\n",
    "        new_y_valid = y_train[i: end]\n",
    "        new_X_train = np.concatenate([X_train[: i], X_train[end:]])\n",
    "        new_y_train = np.concatenate([y_train[: i], y_train[end:]])\n",
    "\n",
    "        l_regression.fit(new_X_train, new_y_train)\n",
    "        prediction = l_regression.predict(new_X_valid)\n",
    "        mse_list.append(error(new_y_valid, prediction))\n",
    "\n",
    "    mean_MSE = np.mean(mse_list)\n",
    "\n",
    "    return mean_MSE\n",
    "    \n",
    "    \n",
    "def gradient_descent(X_train, y_train, lr=0.01, amt_epochs=100):\n",
    "    \"\"\"\n",
    "    lr: learning rate\n",
    "    amt_epochs: cantidad de iteraciones\n",
    "    \n",
    "    shapes: \n",
    "        X_t: nxm\n",
    "        Y_y: nx1\n",
    "        W: mx1\n",
    "    \"\"\"\n",
    "    n = X_train.shape[0]\n",
    "    m = X_train.shape[1]\n",
    "    # print('X.shape:{}x{}\\n'.format(n,m))\n",
    "        \n",
    "    # Inicializamos los pesos\n",
    "    W = np.random.randn(m).reshape(m,1)\n",
    "    print('W_inicial_{}'.format(W.reshape(-1)))\n",
    "    \n",
    "    for i in range(amt_epochs):\n",
    "        # Calculo la estimación\n",
    "        #y_hat=X_train*W\n",
    "        y_hat=np.matmul(X_train,W)\n",
    "        \n",
    "        # Calculo el error\n",
    "        error=y_train-y_hat\n",
    "        \n",
    "        # Calculo el gradiente\n",
    "        grad_sum = np.sum(error*X_train,axis=0)\n",
    "        grad_mul =-2/n*grad_sum  #1xm\n",
    "        gradient = np.transpose(grad_mul).reshape(-1,1) #mx1\n",
    "        \n",
    "        # Actualizo el valor\n",
    "        W = W - (lr*gradient)\n",
    "    \n",
    "    return W\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(X_train, y_train, lr=0.01, amt_epochs=100):\n",
    "    \"\"\"\n",
    "    lr: learning rate\n",
    "    amt_epochs: cantidad de iteraciones\n",
    "    \n",
    "    shapes: \n",
    "        X_t: nxm\n",
    "        Y_y: nx1\n",
    "        W: mx1\n",
    "    \"\"\"\n",
    "    n = X_train.shape[0]\n",
    "    m = X_train.shape[1]\n",
    "    # print('X.shape:{}x{}\\n'.format(n,m))\n",
    "        \n",
    "    # Inicializamos los pesos\n",
    "    W = np.random.randn(m).reshape(m,1)\n",
    "    print('W_inicial_{}'.format(W.reshape(-1)))\n",
    "    \n",
    "    for i in range(amt_epochs):\n",
    "        idx=np.random.permutation(X_train.shape[0])\n",
    "        X_train = X_train[idx]\n",
    "        y_train = y_train[idx]\n",
    "        \n",
    "        for j in range(n):\n",
    "        \n",
    "            # Calculo la estimación\n",
    "            #y_hat=X_train*W\n",
    "            y_hat=np.matmul(X_train[j].reshape(1,-1),W)\n",
    "\n",
    "            # Calculo el error\n",
    "            error=y_train[j]-y_hat\n",
    "\n",
    "            # Calculo el gradiente\n",
    "            grad_sum = error*X_train[j]\n",
    "            grad_mul =-2/n*grad_sum  #1xm\n",
    "            gradient = np.transpose(grad_mul).reshape(-1,1) #mx1\n",
    "\n",
    "            # Actualizo el valor\n",
    "            W = W - (lr*gradient)\n",
    "    \n",
    "    return W\n",
    "\n",
    "def mini_batch_gradient_descent(X_train, y_train, lr=0.01, amt_epochs=100):\n",
    "    \"\"\"\n",
    "    shapes:\n",
    "        X_t = nxm\n",
    "        y_t = nx1\n",
    "        W = mx1\n",
    "    \"\"\"\n",
    "    b = 16\n",
    "    n = X_train.shape[0]\n",
    "    m = X_train.shape[1]\n",
    "\n",
    "    # initialize random weights\n",
    "    W = np.random.randn(m).reshape(m, 1)\n",
    "\n",
    "    for i in range(amt_epochs):\n",
    "        idx = np.random.permutation(X_train.shape[0])\n",
    "        X_train = X_train[idx]\n",
    "        y_train = y_train[idx]\n",
    "\n",
    "        batch_size = int(len(X_train) / b)\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            end = i + batch_size if i + batch_size <= len(X_train) else len(X_train)\n",
    "            batch_X = X_train[i: end]\n",
    "            batch_y = y_train[i: end]\n",
    "\n",
    "            prediction = np.matmul(batch_X, W)  # nx1\n",
    "            error = batch_y - prediction  # nx1\n",
    "\n",
    "            grad_sum = np.sum(error * batch_X, axis=0)\n",
    "            grad_mul = -2/n * grad_sum  # 1xm\n",
    "            gradient = np.transpose(grad_mul).reshape(-1, 1)  # mx1\n",
    "\n",
    "            W = W - (lr * gradient)\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regresión lineal m:[0.7142029909676719] - b:[0.20460785321388442]\n",
      "Regresión cuadrática a:[0.012373848793479192] - b:[0.6035570746552107] - c:[0.4146208287129366]\n",
      "Regresión cuadrática a:[0.012373848793479192] - b:[0.6035570746552107] - c:[0.4146208287129366]\n"
     ]
    }
   ],
   "source": [
    "# Armamos el main\n",
    "#----------------\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Llamo al dataset sobre el que voy a trabajar\n",
    "    #---------------------------------------------\n",
    "    dataset = Data('../income.data.csv')\n",
    "    \n",
    "    # Hacemos la partición del dataset\n",
    "    #---------------------------------\n",
    "    X_train, X_test, y_train, y_test = dataset.split(0.8)\n",
    "\n",
    "    # Llamamos a la regresión lineal (como es un __call__ la llamamos como si fuese función)\n",
    "    #---------------------------------------------------------------------------------------\n",
    "    linear_regression = LinearRegression()\n",
    "    linear_regression.fit(X_train, y_train)\n",
    "    lr_y_hat = linear_regression.predict(X_test)\n",
    "\n",
    "    # Llamamos a la regresión lineal con parámetro \"b\"\n",
    "    #-------------------------------------------------\n",
    "    linear_regression_b = LinearRegressionWithB()\n",
    "    linear_regression_b.fit(X_train, y_train)\n",
    "    lrb_y_hat = linear_regression_b.predict(X_test)\n",
    "    \n",
    "    print('Regresión lineal m:[{}] - b:[{}]'.format(linear_regression_b.model[0],linear_regression_b.model[1]))\n",
    "    \n",
    "    # Llamamos a la regresión cuadrática\n",
    "    #-----------------------------------\n",
    "    quadratic_regression = QuadraticRegression()\n",
    "    quadratic_regression.fit(X_train,y_train)         # Ajuste sobre train data\n",
    "    qrb_y_hat = quadratic_regression.predict(X_test)  # Error sobre test data\n",
    "    \n",
    "    print('Regresión cuadrática a:[{}] - b:[{}] - c:[{}]'.format(quadratic_regression.model[0],quadratic_regression.model[1],quadratic_regression.model[2]))\n",
    "    \n",
    "    # Llamamos a la regresión polinómica\n",
    "    #-----------------------------------\n",
    "    grado = 3\n",
    "    poly_regression = PolyRegression()\n",
    "    poly_regression.fit(X_train,y_train,grado)          # Ajuste sobre train data\n",
    "    poly_y_hat = poly_regression.predict(X_test,grado)  # Error sobre test data\n",
    "    \n",
    "    print('Regresión cuadrática a:[{}] - b:[{}] - c:[{}]'.format(quadratic_regression.model[0],quadratic_regression.model[1],quadratic_regression.model[2]))\n",
    "    \n",
    "    # Hacemos el ajuste contra el modelo constante\n",
    "    #---------------------------------------------\n",
    "    constant_model = ConstantModel()\n",
    "    constant_model.fit(X_train, y_train)\n",
    "    ct_y_hat = constant_model.predict(X_test)\n",
    "\n",
    "    mse = MSE()\n",
    "    lr_mse = mse(y_test, lr_y_hat)\n",
    "    lrb_mse = mse(y_test, lrb_y_hat)\n",
    "    qrb_mse = mse(y_test, qrb_y_hat)\n",
    "    poly_mse = mse(y_test, poly_y_hat)\n",
    "    ct_mse = mse(y_test, ct_y_hat)\n",
    "\n",
    "    # Dibujamos los resultados del ajuste\n",
    "    #------------------------------------\n",
    "    x_plot = np.linspace(0, 10, 10)\n",
    "    lr_y_plot = linear_regression.model * x_plot # m*x\n",
    "    lrb_y_plot = linear_regression_b.model[0] * x_plot + linear_regression_b.model[1]  # m*x + b\n",
    "    qrb_y_plot = quadratic_regression.model[0] * (x_plot**2) + quadratic_regression.model[1] * x_plot + quadratic_regression.model[2]\n",
    "    \n",
    "    poly_y_plot=poly_regression.model[grado]\n",
    "    #print('poly {}:'.format(poly_regression.model))\n",
    "    #print('quad {}:'.format(quadratic_regression.model))\n",
    "    for i in range(grado):\n",
    "        #print(i)\n",
    "        #print(grado-i)\n",
    "        poly_y_plot = poly_y_plot + poly_regression.model[i]*(x_plot**(grado-i))\n",
    "   \n",
    "    plt.scatter(X_train, y_train, color='b', label='dataset')\n",
    "    plt.plot(x_plot, lr_y_plot, color='m', label=f'LinearRegresion(MSE={lr_mse:.3f})')\n",
    "    plt.plot(x_plot, lrb_y_plot, color='r', label=f'LinearRegresionWithB(MSE={lrb_mse:.3f})')\n",
    "    plt.plot(x_plot, qrb_y_plot, color='y', label=f'QuadraticRegresion(MSE={qrb_mse:.3f})')\n",
    "    plt.plot(X_test, ct_y_hat, color='g', label=f'ConstantModel(MSE={ct_mse:.3f})')\n",
    "    plt.plot(x_plot, poly_y_plot, color='k', label=f'PolyRegresion(MSE={poly_mse:.3f})- Grado:{grado:.0f}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conslusiones\n",
    "\n",
    "- No hay practicamente diferencia entre el ajuste lineal (con ordenada al origen) y el cuadrático evaluando el error cuadrátrico medio. Ajustarlo con el modelo cuadrático es hacer un \"overfitting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                Type                     Data/Info\n",
      "----------------------------------------------------------\n",
      "BaseModel               type                     <class '__main__.BaseModel'>\n",
      "ConstantModel           type                     <class '__main__.ConstantModel'>\n",
      "Data                    type                     <class '__main__.Data'>\n",
      "LinearRegression        type                     <class '__main__.LinearRegression'>\n",
      "LinearRegressionWithB   type                     <class '__main__.LinearRegressionWithB'>\n",
      "MSE                     type                     <class '__main__.MSE'>\n",
      "Metric                  type                     <class '__main__.Metric'>\n",
      "PolyRegression          type                     <class '__main__.PolyRegression'>\n",
      "QuadraticRegression     type                     <class '__main__.QuadraticRegression'>\n",
      "X_test                  ndarray                  100: 100 elems, type `float64`, 800 bytes\n",
      "X_train                 ndarray                  398: 398 elems, type `float64`, 3184 bytes\n",
      "dataset                 Data                     <__main__.Data object at 0x000000F1D717BCC8>\n",
      "linear_regression       LinearRegression         <__main__.LinearRegressio<...>ct at 0x000000F1DB2DCFC8>\n",
      "linear_regression_b     LinearRegressionWithB    <__main__.LinearRegressio<...>ct at 0x000000F1DB2DC348>\n",
      "lr_y_hat                ndarray                  100: 100 elems, type `float64`, 800 bytes\n",
      "lrb_y_hat               ndarray                  100: 100 elems, type `float64`, 800 bytes\n",
      "np                      module                   <module 'numpy' from 'C:\\<...>ges\\\\numpy\\\\__init__.py'>\n",
      "plt                     module                   <module 'matplotlib.pyplo<...>\\\\matplotlib\\\\pyplot.py'>\n",
      "poly_regression         PolyRegression           <__main__.PolyRegression <...>ct at 0x000000F1DB2E09C8>\n",
      "qrb_y_hat               ndarray                  100: 100 elems, type `float64`, 800 bytes\n",
      "quadratic_regression    QuadraticRegression      <__main__.QuadraticRegres<...>ct at 0x000000F1DB2DCB08>\n",
      "y_test                  ndarray                  100: 100 elems, type `float64`, 800 bytes\n",
      "y_train                 ndarray                  398: 398 elems, type `float64`, 3184 bytes\n"
     ]
    }
   ],
   "source": [
    "whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparativa entre formulaciones cerradas (regresiones y Gradient Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " GRADIENT DESCENT Vs LINEAR REGRESSION\n",
      "W_inicial_[-2.01501947]\n",
      " W_manual:[0.75397702]\n",
      " W_real:0.7539770177781416\n",
      " Manual time[s]:0.04302716255187988\n",
      "\n",
      "\n",
      "\n",
      " GRADIENT DESCENT Vs LINEAR REGRESSION WITH B\n",
      "W_inicial_[-0.44651334 -0.85301345]\n",
      " W_manual:[0.72667556 0.14079861]\n",
      " W_real:[0.71420299 0.20460785]\n",
      " Manual time[s]:0.6174139976501465\n",
      "\n",
      "\n",
      "\n",
      " STOCHASTIC GRADIENT DESCENT Vs LINEAR REGRESSION WITH B\n",
      "W_inicial_[ 0.57823811 -0.26317163]\n",
      " W_manual:[0.71020781 0.20401224]\n",
      " W_real:[0.71420299 0.20460785]\n",
      " Manual time[s]:15.177178621292114\n",
      "\n",
      "\n",
      "\n",
      "MINI BATCH GRADIENT DESCENT VS LINEAR REGRESSION WITH B\n",
      "W_manual:  [0.70872194 0.20372611]\n",
      "W_real:    [0.71420299 0.20460785]\n",
      "Manual time [s]: 10.104783773422241\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n\\n GRADIENT DESCENT Vs LINEAR REGRESSION')\n",
    "lr_1=0.001\n",
    "amt_epochs_1=1000\n",
    "start = time.time()\n",
    "W_manual = gradient_descent(X_train.reshape(-1,1), y_train.reshape(-1,1), lr=lr_1, amt_epochs=amt_epochs_1)\n",
    "time_1 = time.time()-start\n",
    "W_real = linear_regression.model\n",
    "print(' W_manual:{}\\n W_real:{}\\n Manual time[s]:{}'.format(W_manual.reshape(-1),W_real,time_1))\n",
    "\n",
    "\n",
    "print('\\n\\n\\n GRADIENT DESCENT Vs LINEAR REGRESSION WITH B')\n",
    "X_expanded = np.vstack((X_train,np.ones(len(X_train)))).T\n",
    "lr_2=0.001\n",
    "amt_epochs_2=10000\n",
    "start = time.time()\n",
    "W_manual = gradient_descent(X_expanded, y_train.reshape(-1,1), lr=lr_2, amt_epochs=amt_epochs_2)\n",
    "time_2 = time.time()-start\n",
    "W_real = linear_regression_b.model\n",
    "print(' W_manual:{}\\n W_real:{}\\n Manual time[s]:{}'.format(W_manual.reshape(-1),W_real,time_2))\n",
    "\n",
    "print('\\n\\n\\n STOCHASTIC GRADIENT DESCENT Vs LINEAR REGRESSION WITH B')\n",
    "X_expanded = np.vstack((X_train,np.ones(len(X_train)))).T\n",
    "lr_3=0.05\n",
    "amt_epochs_3=1000\n",
    "start = time.time()\n",
    "W_manual = stochastic_gradient_descent(X_expanded, y_train.reshape(-1,1), lr=lr_3, amt_epochs=amt_epochs_3)\n",
    "time_3 = time.time()-start\n",
    "W_real = linear_regression_b.model\n",
    "print(' W_manual:{}\\n W_real:{}\\n Manual time[s]:{}'.format(W_manual.reshape(-1),W_real,time_3))\n",
    "\n",
    "# gradient descent\n",
    "print('\\n\\n\\nMINI BATCH GRADIENT DESCENT VS LINEAR REGRESSION WITH B')\n",
    "X_expanded = np.vstack((X_train, np.ones(len(X_train)))).T\n",
    "lr_4 = 0.05\n",
    "amt_epochs_4 = 10000\n",
    "start = time.time()\n",
    "W_manual = mini_batch_gradient_descent(X_expanded, y_train.reshape(-1, 1), lr=lr_4, amt_epochs=amt_epochs_4)\n",
    "time_4 = time.time() - start\n",
    "W_real = linear_regression_b.model\n",
    "print('W_manual:  {}\\nW_real:    {}\\nManual time [s]: {}'.format(W_manual.reshape(-1), W_real, time_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
