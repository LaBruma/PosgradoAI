{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de módulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo el módulo de numpy\n",
    "#-------------------------\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levantar datos de disco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  CASO 1 entrada / 1 salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de la clase para levantar (y dividir) los datos\n",
    "# NOTA: Para un solo dato de entrada y una sola salida\n",
    "#===========================================================\n",
    "class Data(object):\n",
    "    def __init__(self, path):\n",
    "        self.dataset = self._build_dataset(path)\n",
    "        \n",
    "    def _build_dataset(self, path):\n",
    "        structure = [('X', np.float),\n",
    "                     ('y', np.float)]\n",
    "\n",
    "        with open(path, encoding=\"utf8\") as data_csv:\n",
    "\n",
    "            data_gen = ((float(line.split(',')[0]), float(line.split(',')[1])) # add here + 10 in second value\n",
    "                        for i, line in enumerate(data_csv) if i != 0)\n",
    "            embeddings = np.fromiter(data_gen, structure)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def split(self, percentage): # 0.8\n",
    "        X = self.dataset['X']\n",
    "        y = self.dataset['y']\n",
    "\n",
    "        # X.shape[0] -> 10 (filas)\n",
    "\n",
    "        permuted_idxs = np.random.permutation(X.shape[0])\n",
    "\n",
    "        train_idxs = permuted_idxs[0:int(percentage * X.shape[0])]\n",
    "\n",
    "        test_idxs = permuted_idxs[int(percentage * X.shape[0]): X.shape[0]]\n",
    "                    #[9,0]\n",
    "\n",
    "        X_train = X[train_idxs]\n",
    "        X_test = X[test_idxs]\n",
    "\n",
    "        y_train = y[train_idxs]\n",
    "        y_test = y[test_idxs]\n",
    "\n",
    "        return X_train, X_test, y_train, y_test  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CASO 2 entradas / 1 salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de la clase para levantar (y dividir) los datos\n",
    "# NOTA: Para dos datos de entrada y una sola salida\n",
    "#===========================================================\n",
    "class Data(object):\n",
    "    def __init__(self, path):\n",
    "        self.dataset = self._build_dataset(path)\n",
    "    \n",
    "    def _build_dataset(self, path):\n",
    "        # Armo una estructura de datos para guardarlos ahí\n",
    "        #-------------------------------------------------\n",
    "        structure = [('X1', np.float),\n",
    "                     ('X2', np.float),\n",
    "                     ('y', np.int)]\n",
    "        \n",
    "        # Abro el archivo lo recorro llenando la estructura creada línea a línea\n",
    "        #-----------------------------------------------------------------------\n",
    "        with open(path, encoding=\"utf8\") as data_csv:\n",
    "            # A la estructura (data_gen) hay que armarla según lo que figure en el archivo\n",
    "            data_gen = ((float(line.split(',')[0]), float(line.split(',')[1]), np.int(line.split(',')[2])) # add here + 10 in second value\n",
    "                        for i, line in enumerate(data_csv) if i != 0)\n",
    "            embeddings = np.fromiter(data_gen, structure)\n",
    "\n",
    "        return embeddings\n",
    "    \n",
    "    # Separo los los datos (train y test)\n",
    "    #------------------------------------\n",
    "    def split(self, percentage): # 0.8 en general\n",
    "        X = self.dataset[['X1', 'X2']]\n",
    "        y = self.dataset['y']\n",
    "        \n",
    "        # Permutamos los datos de entrada (asumimos que vienen en columna)\n",
    "        # Es decir, cada fila sería una ocurrencia distinta\n",
    "        #-----------------------------------------------------------------\n",
    "        permuted_idxs = np.random.permutation(len(X)) #len me da la dimensión mayor\n",
    "        \n",
    "        # Separamos en Train (80%) y Test (20%)\n",
    "        #--------------------------------------\n",
    "        train_idxs = permuted_idxs[0:int(percentage * X.shape[0])]\n",
    "        test_idxs = permuted_idxs[int(percentage * X.shape[0]): X.shape[0]]\n",
    "\n",
    "        X_train = X[train_idxs]\n",
    "        X_test = X[test_idxs]\n",
    "\n",
    "        y_train = y[train_idxs]\n",
    "        y_test = y[test_idxs]\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "        # Separamos en Train (70%), Test (20%) y Validation (10%)\n",
    "        #--------------------------------------------------------\n",
    "        #... train_idxs = permuted_idxs[0:int(percentage * X1.shape[0])]\n",
    "#         test_idxs = permuted_idxs[int(percentage * X1.shape[0]):int(0.9 * X1.shape[0])]\n",
    "#         valid_idxs = permuted_idxs[int(0.9 * X1.shape[0]):X1.shape[0]]\n",
    "\n",
    "#         X_train = np.vstack((X1[train_idxs],X2[train_idxs]))\n",
    "#         X_test = np.vstack((X1[test_idxs],X2[test_idxs]))\n",
    "#         X_valid = np.vstack((X1[valid_idxs],X2[valid_idxs]))\n",
    "\n",
    "#         y_train = y[train_idxs]\n",
    "#         y_test = y[test_idxs]\n",
    "#         y_valid = y[valid_idxs]\n",
    "        \n",
    "#         return X_train, X_test, X_valid, y_train, y_test, y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que realiza prepara los datos e itera llamando a k_means_loop (la que hace las cuentas)\n",
    "#------------------------------------------------------------------------------------------------\n",
    "def k_means(X, n_clusters):\n",
    "    # Armamos una matriz identidad con tamaño cantidad de centroides por cantidad de columnas de X\n",
    "    centroids = np.eye(n_clusters, X.shape[1])\n",
    "    print(centroids)\n",
    "    # Para la cantidad de iteraciones indicadas\n",
    "    for i in range(MAX_ITERATIONS):\n",
    "        print(\"Iteration # {}\".format(i))\n",
    "        # Corremos el algoritmo de k-means\n",
    "        centroids, cluster_ids = k_means_loop(X, centroids)\n",
    "        print(centroids)\n",
    "    return centroids, cluster_ids\n",
    "\n",
    "# Este es el loop que realiza los cálculos\n",
    "def k_means_loop(X, centroids):\n",
    "    # Encontramos etiquetas para las filas en X basado en los valores de los centroides\n",
    "    expanded_centroids = centroids[:, None]\n",
    "    distances = np.sqrt(np.sum((expanded_centroids - X) ** 2, axis=2))  # Calculo las distancias (norma 2) de los centroides a cada vector de X\n",
    "    arg_min = np.argmin(distances, axis=0) # Devuelve los índices de los vectores con menor distancia a los centroides\n",
    "    \n",
    "    # Recalcular los centroides\n",
    "    for i in range(centroids.shape[0]):\n",
    "        centroids[i] = np.mean(X[arg_min == i, :], axis=0)\n",
    "    \n",
    "    return centroids, arg_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA (por las dudas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-d456a3af43b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Ahora comparamos contra la función embebida de PCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Ahora comparamos contra la función embebida de PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "x_std = StandardScaler(with_std=False).fit_transform(x)\n",
    "pca.fit_transform(x_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresiones lineales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase base de la que heredan las que vayamos implementando\n",
    "#-----------------------------------------------------------\n",
    "# Es conveniente tener una clase base de la que vayan heredando las demás. Siempre habrá un método fit\n",
    "# y un método predict. Pero en esta clase base puede haber definiciones de atributos comunes a todas\n",
    "#===========================================================\n",
    "class BaseModel(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        return NotImplemented\n",
    "\n",
    "    def predict(self, X):\n",
    "        return NotImplemented\n",
    "\n",
    "\n",
    "class ConstantModel(BaseModel):\n",
    "    # El modelo constante solo saca la media de los datos y devuelve ese valor\n",
    "    # Es útil para comparar. Ningún modelo debería ser peor que este.\n",
    "    #-------------------------------------------------------------------------\n",
    "    def fit(self, X, Y):\n",
    "        W = Y.mean()\n",
    "        self.model = W\n",
    "\n",
    "    def predict(self, X):\n",
    "        # La \"predicción\" consiste en devolver la media para todos los valores\n",
    "        return np.ones(len(X)) * self.model\n",
    "\n",
    "# Modelo de la regresión lineal\n",
    "#==============================\n",
    "class LinearRegression(BaseModel):\n",
    "    # Este modelo de regresión lineal ajusta únicamente la pendiente, no contempla la ordenada al origen\n",
    "    def fit(self, X, y):\n",
    "        # Verificamos si X es un vector o una matriz\n",
    "        if len(X.shape) == 1:\n",
    "            # Esta es una manera de escribir la pseudo-inversa (X'.X)^(-1).X'.y\n",
    "            W = X.T.dot(y) / X.T.dot(X)\n",
    "        else:\n",
    "            # Y esta es la manera con matrices\n",
    "            W = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "        self.model = W\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X.dot(self.model)\n",
    "    \n",
    "# Modelo que incluye la ordenada al origen (b)\n",
    "# ============================================\n",
    "class LinearRegressionWithB(BaseModel):\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # En el caso de ajustar con ordenada al origen le agregamos la columna de b con unos\n",
    "        # (Le agrega la fila abajo y luego traspongo --> Vectores columna)\n",
    "        X_expanded = np.vstack((X, np.ones(len(X)))).T\n",
    "        W = np.linalg.inv(X_expanded.T.dot(X_expanded)).dot(X_expanded.T).dot(y)\n",
    "        self.model = W\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_expanded = np.vstack((X, np.ones(len(X)))).T\n",
    "        return X_expanded.dot(self.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradientes descendentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, lr=0.01, amt_epochs=100):\n",
    "    \"\"\"\n",
    "    lr: learning rate\n",
    "    amt_epochs: cantidad de iteraciones\n",
    "    \n",
    "    shapes: \n",
    "        X_t: nxm\n",
    "        Y_y: nx1\n",
    "        W: mx1\n",
    "    \"\"\"\n",
    "    n = X_train.shape[0]\n",
    "    m = X_train.shape[1]\n",
    "    # print('X.shape:{}x{}\\n'.format(n,m))\n",
    "        \n",
    "    # Inicializamos los pesos\n",
    "    W = np.random.randn(m).reshape(m,1)\n",
    "    print('W_inicial_{}'.format(W.reshape(-1)))\n",
    "    \n",
    "    for i in range(amt_epochs):\n",
    "        # Calculo la estimación\n",
    "        #y_hat=X_train*W\n",
    "        y_hat=np.matmul(X_train,W)\n",
    "        \n",
    "        # Calculo el error\n",
    "        error=y_train-y_hat\n",
    "        \n",
    "        # Calculo el gradiente\n",
    "        grad_sum = np.sum(error*X_train,axis=0)\n",
    "        grad_mul =-2/n*grad_sum  #1xm\n",
    "        gradient = np.transpose(grad_mul).reshape(-1,1) #mx1\n",
    "        \n",
    "        # Actualizo el valor\n",
    "        W = W - (lr*gradient)\n",
    "    \n",
    "    return W\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(X_train, y_train, lr=0.01, amt_epochs=100):\n",
    "    \"\"\"\n",
    "    lr: learning rate\n",
    "    amt_epochs: cantidad de iteraciones\n",
    "    \n",
    "    shapes: \n",
    "        X_t: nxm\n",
    "        Y_y: nx1\n",
    "        W: mx1\n",
    "    \"\"\"\n",
    "    n = X_train.shape[0]\n",
    "    m = X_train.shape[1]\n",
    "    # print('X.shape:{}x{}\\n'.format(n,m))\n",
    "        \n",
    "    # Inicializamos los pesos\n",
    "    W = np.random.randn(m).reshape(m,1)\n",
    "    print('W_inicial_{}'.format(W.reshape(-1)))\n",
    "    \n",
    "    for i in range(amt_epochs):\n",
    "        idx=np.random.permutation(X_train.shape[0])\n",
    "        X_train = X_train[idx]\n",
    "        y_train = y_train[idx]\n",
    "        \n",
    "        for j in range(n):\n",
    "        \n",
    "            # Calculo la estimación\n",
    "            #y_hat=X_train*W\n",
    "            y_hat=np.matmul(X_train[j].reshape(1,-1),W)\n",
    "\n",
    "            # Calculo el error\n",
    "            error=y_train[j]-y_hat\n",
    "\n",
    "            # Calculo el gradiente\n",
    "            grad_sum = error*X_train[j]\n",
    "            grad_mul =-2/n*grad_sum  #1xm\n",
    "            gradient = np.transpose(grad_mul).reshape(-1,1) #mx1\n",
    "\n",
    "            # Actualizo el valor\n",
    "            W = W - (lr*gradient)\n",
    "    \n",
    "    return W\n",
    "\n",
    "def mini_batch_gradient_descent(X_train, y_train, lr=0.01, amt_epochs=100):\n",
    "    \"\"\"\n",
    "    shapes:\n",
    "        X_t = nxm\n",
    "        y_t = nx1\n",
    "        W = mx1\n",
    "    \"\"\"\n",
    "    b = 16\n",
    "    n = X_train.shape[0]\n",
    "    m = X_train.shape[1]\n",
    "    # initialize random weights\n",
    "    W = np.random.randn(m).reshape(m, 1)\n",
    "\n",
    "    for i in range(amt_epochs):\n",
    "        idx = np.random.permutation(X_train.shape[0])\n",
    "        X_train = X_train[idx]\n",
    "        y_train = y_train[idx]\n",
    "\n",
    "        batch_size = int(len(X_train) / b)\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            end = i + batch_size if i + batch_size <= len(X_train) else len(X_train)\n",
    "            batch_X = X_train[i: end]\n",
    "            batch_y = y_train[i: end]\n",
    "\n",
    "            prediction = np.matmul(batch_X, W)  # nx1\n",
    "            error = batch_y - prediction  # nx1\n",
    "\n",
    "            grad_sum = np.sum(error * batch_X, axis=0)\n",
    "            grad_mul = -2/n * grad_sum  # 1xm\n",
    "            gradient = np.transpose(grad_mul).reshape(-1, 1)  # mx1\n",
    "\n",
    "            W = W - (lr * gradient)\n",
    "\n",
    "    return W\n",
    "\n",
    "\n",
    "def mini_batch_logistic_regression(X_train, y_train, lr=0.01, amt_epochs=100):\n",
    "    \"\"\"\n",
    "    shapes:\n",
    "        X_t = nxm\n",
    "        y_t = nx1\n",
    "        W = mx1\n",
    "    \"\"\"\n",
    "    b = 16\n",
    "    n = X_train.shape[0]\n",
    "    m = X_train.shape[1]\n",
    "\n",
    "    # initialize random weights\n",
    "    W = np.random.randn(m).reshape(m, 1)\n",
    "\n",
    "    for i in range(amt_epochs):\n",
    "        idx = np.random.permutation(X_train.shape[0])\n",
    "        X_train = X_train[idx]\n",
    "        y_train = y_train[idx]\n",
    "\n",
    "        batch_size = int( len(X_train) / b)\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            end = i + batch_size if i + batch_size <= len(X_train) else len(X_train)\n",
    "            batch_X = X_train[i: end]\n",
    "            batch_y = y_train[i: end]\n",
    "            prediction = 1/(1+np.exp(-np.matmul(batch_X, W))) #Ojo que no es la predicción posta!!\n",
    "            error = batch_y - prediction  # nx1\n",
    "\n",
    "            grad_sum = np.sum(error * batch_X, axis=0)\n",
    "            grad_mul = -2/b * grad_sum  # 1xm\n",
    "            gradient = np.transpose(grad_mul).reshape(-1, 1)  # mx1\n",
    "\n",
    "            W = W - (lr * gradient)\n",
    "\n",
    "    return W\n",
    "\n",
    "def sin_fitting_example():\n",
    "    # y = sin(x)\n",
    "    amt_points = 36\n",
    "    x = np.linspace(0, 360, num=amt_points)\n",
    "    y = np.sin(x * np.pi / 180.)\n",
    "    noise = np.random.normal(0, .1, y.shape)\n",
    "    noisy_y = y + noise\n",
    "\n",
    "    X_train = x\n",
    "    y_train = noisy_y\n",
    "\n",
    "    regression = LinearRegression()\n",
    "\n",
    "    # linear\n",
    "    X_linear = np.vstack((X_train, np.ones(len(X_train)))).T\n",
    "    regression.fit(X_linear, y_train.reshape(-1, 1))\n",
    "    W_linear = regression.model\n",
    "    y_linear = W_linear[0]*x + W_linear[1]\n",
    "\n",
    "    # quadratic\n",
    "    X_quadratic = np.vstack((np.power(X_train, 2), X_train, np.ones(len(X_train)))).T\n",
    "    regression.fit(X_quadratic, y_train.reshape(-1, 1))\n",
    "    W_quadratic = regression.model\n",
    "    y_quadratic = W_quadratic[0] * np.power(x, 2) + W_quadratic[1] * x + W_quadratic[2]\n",
    "\n",
    "    # cubic\n",
    "    X_cubic = np.vstack((np.power(X_train, 3), np.power(X_train, 2), X_train, np.ones(len(X_train)))).T\n",
    "    regression.fit(X_cubic, y_train.reshape(-1, 1))\n",
    "    W_cubic = regression.model\n",
    "    y_cubic = W_cubic[0] * np.power(x, 3) + W_cubic[1] * np.power(x, 2) + W_cubic[2] * x + W_cubic[3]\n",
    "\n",
    "    # X10\n",
    "    X_10 = np.vstack((np.power(X_train, 10), np.power(X_train, 9), np.power(X_train, 8),\n",
    "                      np.power(X_train, 7), np.power(X_train, 6), np.power(X_train, 5),\n",
    "                      np.power(X_train, 4), np.power(X_train, 3), np.power(X_train, 2),\n",
    "                      X_train, np.ones(len(X_train)))).T\n",
    "    regression.fit(X_10, y_train.reshape(-1, 1))\n",
    "    W_10 = regression.model\n",
    "    y_10 = W_10[0] * np.power(x, 10) + W_10[1] * np.power(x, 9) + W_10[2] * np.power(x, 8) + \\\n",
    "           W_10[3] * np.power(x, 7) + W_10[4] * np.power(x, 6) + W_10[5] * np.power(x, 5) + \\\n",
    "           W_10[6] * np.power(x, 4) + W_10[7] * np.power(x, 3) + W_10[8] * np.power(x, 2) + \\\n",
    "           W_10[9] * x + W_10[10]\n",
    "\n",
    "    # PLOTS\n",
    "    plt.figure()\n",
    "    plt.subplot(1, 1, 1)\n",
    "    plt.gca().set_title('Sin(x) - Fitting curves')\n",
    "\n",
    "    # original\n",
    "    plt.plot(x, noisy_y, 'o')\n",
    "\n",
    "    # linear\n",
    "    plt.plot(x, y_linear, '-')\n",
    "\n",
    "    # quadratic\n",
    "    plt.plot(x, y_quadratic, '-')\n",
    "\n",
    "    # cubic\n",
    "    plt.plot(x, y_cubic, '-')\n",
    "\n",
    "    # 10 power\n",
    "    plt.plot(x, y_10, '-')\n",
    "\n",
    "    plt.legend(['noisy signal', 'linear', 'quadratic', 'cubic', '10th power'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_folds(X_train, y_train, k=5):\n",
    "    l_regression = LinearRegression()\n",
    "    error = MSE()\n",
    "\n",
    "    chunk_size = int(len(X_train) / k)\n",
    "    mse_list = []\n",
    "    for i in range(0, len(X_train), chunk_size):\n",
    "        end = i + chunk_size if i + chunk_size <= len(X_train) else len(X_train)\n",
    "        new_X_valid = X_train[i: end]\n",
    "        new_y_valid = y_train[i: end]\n",
    "        new_X_train = np.concatenate([X_train[: i], X_train[end:]])\n",
    "        new_y_train = np.concatenate([y_train[: i], y_train[end:]])\n",
    "\n",
    "        l_regression.fit(new_X_train, new_y_train)\n",
    "        prediction = l_regression.predict(new_X_valid)\n",
    "        mse_list.append(error(new_y_valid, prediction))\n",
    "\n",
    "    mean_MSE = np.mean(mse_list)\n",
    "\n",
    "    return mean_MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clases de métricas\n",
    "#===================\n",
    "\n",
    "# Clase madre\n",
    "class Metric(object):\n",
    "    def __call__(self, target, prediction):\n",
    "        return NotImplemented\n",
    "\n",
    "# Por ahora solo esta --> Error cuadrático medio\n",
    "class MSE(Metric):\n",
    "    def __init__(self):\n",
    "        Metric.__init__(self)\n",
    "        \n",
    "    def __call__(self, target, prediction):\n",
    "        n = target.size\n",
    "        return np.sum((target - prediction) ** 2) / n\n",
    "\n",
    "class Precision(Metric):\n",
    "    def __init__(self):\n",
    "        Metric.__init__(self)\n",
    "        \n",
    "    def __call__(self, truth, prediction):\n",
    "        \n",
    "        # Encontramos los True Positive\n",
    "        true_pos_mask = (prediction == 1) & (truth == 1)\n",
    "        true_pos = true_pos_mask.sum()\n",
    "        \n",
    "        # Encontramos los False Positive\n",
    "        false_pos_mask = (prediction == 1) & (truth == 0)\n",
    "        false_pos = false_pos_mask.sum()\n",
    "        \n",
    "        return true_pos / (true_pos + false_pos)\n",
    "\n",
    "class Recall (Metric):\n",
    "    def __init__(self):\n",
    "        Metric.__init__(self)\n",
    "        \n",
    "    def __call__(self, truth, prediction):\n",
    "        \n",
    "        # Encontramos los True Positive\n",
    "        true_pos_mask = (prediction == 1) & (truth == 1)\n",
    "        true_pos = true_pos_mask.sum()\n",
    "        \n",
    "        # Encontramos los False Negative\n",
    "        false_neg_mask = (prediction == 0) & (truth == 1)\n",
    "        false_neg = false_neg_mask.sum()\n",
    "        \n",
    "        return true_pos / (true_pos + false_neg)\n",
    "        \n",
    "class Accuracy (Metric):\n",
    "    def __init__(self):\n",
    "        Metric.__init__(self)\n",
    "        \n",
    "    def __call__(self, truth, prediction):\n",
    "        \n",
    "        # Encontramos los True Positive\n",
    "        true_pos_mask = (prediction == 1) & (truth == 1)\n",
    "        true_pos = true_pos_mask.sum()\n",
    "        \n",
    "        # Encontramos los False Positive\n",
    "        false_pos_mask = (prediction == 1) & (truth == 0)\n",
    "        false_pos = false_pos_mask.sum()\n",
    "        \n",
    "        # Encontramos los True Negative\n",
    "        true_neg_mask = (prediction == 0) & (truth == 0)\n",
    "        true_neg = true_neg_mask.sum() \n",
    "    \n",
    "        # Encontramos los False Negative\n",
    "        false_neg_mask = (prediction == 0) & (truth == 1)\n",
    "        false_neg = false_neg_mask.sum()\n",
    "    \n",
    "        return (true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examen Final - Intro a AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Punto 2\n",
    "\n",
    "- a) Obtener el dataset desde el siguiente link. La primera columna representa los datos de entrada y la segunda columna representa los datos de salida.\n",
    "\n",
    "- b) Levantar el dataset en un arreglo de Numpy. \n",
    "\n",
    "- c) Graficar el dataset de manera tal que sea posible visualizar la nube de puntos.\n",
    "\n",
    "- d) Partir el dataset en train (80%) y test (20%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# b) Levantamos en un arreglo Numpy los datos\n",
    "dataset = Data('../clase_8_dataset.csv')\n",
    "# c) Para mostrarlos todos hago un \"split mentiroso\" al 100%. Todos los datos quedan en Train\n",
    "X_train, X_test, y_train, y_test = dataset.split(1)\n",
    "# Graficamos...\n",
    "plt.scatter(X_train, y_train, s=10, label='Dataset')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# d) Ahora sí particionamos el dataset en train (80%) y test (20%)\n",
    "X_train, X_test, y_train, y_test = dataset.split(0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Punto 3\n",
    "\n",
    "Utilizar regresión polinómica para hacer “fit” sobre la nube de puntos del train. Para este ejercicio, se desea utilizar la fórmula cerrada de la optimización polinómica. El modelo es de la forma y = [Wn … W0] * [X^n    X^(n-1)    …    1]. \n",
    "\n",
    "- a) Para n = 1 (modelo lineal con ordenada al origen), hacer un fit del modelo utilizando K-FOLDS. Para K-FOLDS partir el train dataset en 5 partes iguales, utilizar 4/5 para entrenar y 1/5 para validar. Informar el mejor modelo obtenido y el criterio utilizado para elegir dicho modelo (dejar comentarios en el código).\n",
    "\n",
    "- b) Repetir el punto (a), para n = {2,3,4}. Computar el error de validación y test del mejor modelo para cada n.\n",
    "\n",
    "- c) Elegir el polinomio que hace mejor fit sobre la nube de puntos y explicar el criterio seleccionado (dejar comentarios en el código). \n",
    "\n",
    "- d) Graficar el polinomio obtenido y el dataset de test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error de ajuste n=1: 925.676671178207\n",
      "Error de ajuste n=2: 101.69352938450034\n",
      "Error de ajuste n=3: 8.739423453141244\n",
      "Error de ajuste n=4: 8.776601235461433\n"
     ]
    }
   ],
   "source": [
    "# a) K-folds para el caso lineal\n",
    "# n=1: Caso Lineal\n",
    "X_linear = np.vstack((X_train, np.ones(len(X_train)))).T\n",
    "mean_MSE = k_folds(X_linear, y_train.reshape(-1, 1), k=5)\n",
    "print(\"Error de ajuste n=1: {}\".format(mean_MSE))\n",
    "\n",
    "# b) K-folds para los demás órdenes\n",
    "# n=2: Caso cuadratico\n",
    "X_quadratic = np.vstack((np.power(X_train, 2), X_train, np.ones(len(X_train)))).T\n",
    "mean_MSE = k_folds(X_quadratic, y_train, k=5)\n",
    "print(\"Error de ajuste n=2: {}\".format(mean_MSE))\n",
    "\n",
    "# n=3: Caso cúbico\n",
    "X_cubic = np.vstack((np.power(X_train, 3), np.power(X_train, 2), X_train, np.ones(len(X_train)))).T\n",
    "mean_MSE = k_folds(X_cubic, y_train, k=5)\n",
    "print(\"Error de ajuste n=3: {}\".format(mean_MSE))\n",
    "\n",
    "# n=4: Caso poly 4\n",
    "X_4 = np.vstack((np.power(X_train, 4), np.power(X_train, 3), np.power(X_train, 2), X_train, np.ones(len(X_train)))).T\n",
    "mean_MSE = k_folds(X_4, y_train, k=5)\n",
    "print(\"Error de ajuste n=4: {}\".format(mean_MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Conclusión:: \n",
    "De estos resultados podemos ver que el error de ajuste para n=3 es el más pequeño (inclusive mejor que n=4). Tomamos entonces n=3 para seguir. Todo esto está basado en la técnica de K-folds que va partiendo el train dataset en las partes que se le indique (en nuestro caso k=5) y encontrando un promedio de los errores cuadráticos medios entrenando sobre 4/5 de las particiones y validando sobre 1/5, para todas las combinaciones.\n",
    "\n",
    "En resumen, como dijimos, tomamos n=3 para seguir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# d) Graficamos el polinomio y el dataset de test\n",
    "# n=3: Caso cúbico\n",
    "X_cubic = np.vstack((np.power(X_train, 3), np.power(X_train, 2), X_train, np.ones(len(X_train)))).T\n",
    "\n",
    "x = np.linspace(-400, 400, num=800)\n",
    "\n",
    "regression = LinearRegression()\n",
    "regression.fit(X_cubic, y_train.reshape(-1, 1))\n",
    "W_cubic = regression.model\n",
    "y_cubic = W_cubic[0] * np.power(x, 3) + W_cubic[1] * np.power(x, 2) + W_cubic[2] * x + W_cubic[3]\n",
    "\n",
    "# Graficamos\n",
    "plt.scatter(X_test, y_test, s=10, label='Test data')\n",
    "plt.plot(x, y_cubic, 'r-')\n",
    "plt.legend(['Ajuste (n=3)','Test data'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Punto 4: Para el mejor modelo seleccionado en (3c) (el mejor “n”), hacer la optimización utilizando Mini-Batch Gradient Descent (partir el train dataset en 4/5 para entrenar y 1/5 para validar).\n",
    "\n",
    "- a) Para cada epoch, calcular el error de train y el error de validation.\n",
    "- b) Graficar el error de train y el error de validación en función del número de epoch.\n",
    "- c) Comparar los resultados obtenidos para el modelo entrenado con Mini-Batch, contra el modelo obtenido en (3c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_manual:  [-0.072758   48.32541884]\n",
      "W_real:    [[-0.08689686 49.16033235]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Con el polinomio de grado 3 se complica la convergencia...usamos un de grado 1\n",
    "\n",
    "X_linear = np.vstack((X_train, np.ones(len(X_train)))).T\n",
    "regression = LinearRegression()\n",
    "regression.fit(X_linear, y_train.reshape(-1, 1))\n",
    "W_linear = regression.model\n",
    "\n",
    "lr_4 = 0.0002\n",
    "amt_epochs = 10000\n",
    "W_manual = mini_batch_gradient_descent(X_linear, y_train.reshape(-1, 1), lr=lr_4, amt_epochs=amt_epochs)\n",
    "W_real = W_linear.T\n",
    "print('W_manual:  {}\\nW_real:    {}\\n'.format(W_manual.reshape(-1), W_real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================\n",
    "# Código que tenía para pruebas\n",
    "#==============================\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     dataset = Data('../income.data.csv')\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = dataset.split(0.8)\n",
    "\n",
    "#     linear_regression = LinearRegression()\n",
    "#     linear_regression.fit(X_train, y_train)\n",
    "#     lr_y_hat = linear_regression.predict(X_test)\n",
    "\n",
    "#     linear_regression_b = LinearRegressionWithB()\n",
    "#     linear_regression_b.fit(X_train, y_train)\n",
    "#     lrb_y_hat = linear_regression_b.predict(X_test)\n",
    "\n",
    "#     constant_model = ConstantModel()\n",
    "#     constant_model.fit(X_train, y_train)\n",
    "#     ct_y_hat = constant_model.predict(X_test)\n",
    "\n",
    "#     mse = MSE()\n",
    "#     lr_mse = mse(y_test, lr_y_hat)\n",
    "#     lrb_mse = mse(y_test, lrb_y_hat)\n",
    "#     ct_mse = mse(y_test, ct_y_hat)\n",
    "\n",
    "#     x_plot = np.linspace(0, 10, 10)\n",
    "#     lr_y_plot = linear_regression.model * x_plot\n",
    "#     lrb_y_plot = linear_regression_b.model[0] * x_plot + linear_regression_b.model[1]\n",
    "\n",
    "#     # gradient descent\n",
    "#     print('\\n\\n\\nGRADIENT DESCENT VS LINEAR REGRESSION')\n",
    "#     lr_1 = 0.001\n",
    "#     amt_epochs_1 = 1000\n",
    "#     start = time.time()\n",
    "#     W_manual = gradient_descent(X_train.reshape(-1, 1), y_train.reshape(-1, 1), lr=lr_1, amt_epochs=amt_epochs_1)\n",
    "#     time_1 = time.time() - start\n",
    "#     W_real = linear_regression.model\n",
    "#     print('W_manual:  {}\\nW_real:    {}\\nManual time [s]: {}'.format(W_manual.reshape(-1), W_real, time_1))\n",
    "\n",
    "#     # gradient descent\n",
    "#     print('\\n\\n\\nGRADIENT DESCENT VS LINEAR REGRESSION WITH B')\n",
    "#     X_expanded = np.vstack((X_train, np.ones(len(X_train)))).T\n",
    "#     lr_2 = 0.001\n",
    "#     amt_epochs_2 = 100000\n",
    "#     start = time.time()\n",
    "#     W_manual = gradient_descent(X_expanded, y_train.reshape(-1, 1), lr=lr_2, amt_epochs=amt_epochs_2)\n",
    "#     time_2 = time.time() - start\n",
    "#     W_real = linear_regression_b.model\n",
    "#     print('W_manual:  {}\\nW_real:    {}\\nManual time [s]: {}'.\n",
    "#           format(W_manual.reshape(-1), W_real, time_2))\n",
    "\n",
    "#     # gradient descent\n",
    "#     print('\\n\\n\\nSTOCHASTIC GRADIENT DESCENT VS LINEAR REGRESSION WITH B')\n",
    "#     X_expanded = np.vstack((X_train, np.ones(len(X_train)))).T\n",
    "#     lr_3 = 0.05\n",
    "#     amt_epochs_3 = 1000\n",
    "#     start = time.time()\n",
    "#     W_manual = stochastic_gradient_descent(X_expanded, y_train.reshape(-1, 1), lr=lr_3, amt_epochs=amt_epochs_3)\n",
    "#     time_3 = time.time() - start\n",
    "#     W_real = linear_regression_b.model\n",
    "#     print('W_manual:  {}\\nW_real:    {}\\nManual time [s]: {}'.\n",
    "#           format(W_manual.reshape(-1), W_real, time_3))\n",
    "\n",
    "#     # gradient descent\n",
    "#     print('\\n\\n\\nMINI BATCH GRADIENT DESCENT VS LINEAR REGRESSION WITH B')\n",
    "#     X_expanded = np.vstack((X_train, np.ones(len(X_train)))).T\n",
    "#     lr_4 = 0.05\n",
    "#     amt_epochs_4 = 10000\n",
    "#     start = time.time()\n",
    "#     W_manual = mini_batch_gradient_descent(X_expanded, y_train.reshape(-1, 1), lr=lr_4, amt_epochs=amt_epochs_4)\n",
    "#     time_4 = time.time() - start\n",
    "#     W_real = linear_regression_b.model\n",
    "#     print('W_manual:  {}\\nW_real:    {}\\nManual time [s]: {}'.\n",
    "#           format(W_manual.reshape(-1), W_real, time_4))\n",
    "\n",
    "#     # PLOTS\n",
    "#     plt.figure()\n",
    "#     x_plot = np.linspace(1, 4, 4)\n",
    "#     legend = ['GD', 'GD(B)', 'S-GD(B)', 'MB-GD(B)']\n",
    "\n",
    "#     plt.subplot(1, 3, 1)\n",
    "#     plt.gca().set_title('Learning Rate')\n",
    "#     y_plot = [lr_1, lr_2, lr_3, lr_4]\n",
    "#     plt.plot(x_plot[0], y_plot[0], 'o', x_plot[1], y_plot[1], 'o', x_plot[2], y_plot[2], 'o',\n",
    "#              x_plot[3], y_plot[3], 'o')\n",
    "#     plt.legend(legend)\n",
    "#     for x, y in zip(x_plot, y_plot):\n",
    "#         plt.text(x, y, str(y))\n",
    "\n",
    "#     plt.subplot(1, 3, 2)\n",
    "#     plt.gca().set_title('Epochs')\n",
    "#     y_plot = [amt_epochs_1, amt_epochs_2, amt_epochs_3, amt_epochs_4]\n",
    "#     plt.plot(x_plot[0], y_plot[0], 'o', x_plot[1], y_plot[1], 'o', x_plot[2], y_plot[2], 'o',\n",
    "#              x_plot[3], y_plot[3], 'o')\n",
    "#     plt.legend(legend)\n",
    "#     for x, y in zip(x_plot, y_plot):\n",
    "#         plt.text(x, y, str(y))\n",
    "\n",
    "#     plt.subplot(1, 3, 3)\n",
    "#     plt.gca().set_title('Time')\n",
    "#     y_plot = [time_1, time_2, time_3, time_4]\n",
    "#     plt.plot(x_plot[0], y_plot[0], 'o', x_plot[1], y_plot[1], 'o', x_plot[2], y_plot[2], 'o',\n",
    "#              x_plot[3], y_plot[3], 'o')\n",
    "#     plt.legend(legend)\n",
    "#     for x, y in zip(x_plot, y_plot):\n",
    "#         plt.text(x, y, str(y))\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "#     # sin fitting example\n",
    "#     sin_fitting_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded\n",
      "Dataset split\n",
      "Training\n",
      "W: [  0.12245323   0.11537612 -13.68000208]\n",
      "Time [s]: 50.46697926521301\n"
     ]
    }
   ],
   "source": [
    "#==============================\n",
    "# Código que tenía para pruebas\n",
    "#==============================\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     dataset = Data('../clase_6_dataset.txt')\n",
    "#     print('Dataset loaded')\n",
    "#     X_train, X_test, y_train, y_test = dataset.split(1)\n",
    "#     print('Dataset split')\n",
    "\n",
    "#     # gradient descent (es importante que tenga la columna de unos)\n",
    "#     X_expanded = np.vstack((X_train['X1'], X_train['X2'], np.ones(len(X_train)))).T\n",
    "#     lr = 0.001\n",
    "#     amt_epochs = 50000\n",
    "#     print('Training')\n",
    "#     start = time.time()\n",
    "#     W = mini_batch_logistic_regression(X_expanded, y_train.reshape(-1, 1), lr=lr,amt_epochs=amt_epochs)\n",
    "#     time = time.time() - start\n",
    "#     print('W: {}\\nTime [s]: {}'.format(W.reshape(-1), time))\n",
    "\n",
    "#     # PLOTS\n",
    "#     # filter out the applicants that got admitted\n",
    "#     admitted = X_train[y_train == 1]\n",
    "#     # filter out the applicants that didn't get admission\n",
    "#     not_admitted = X_train[y_train == 0]\n",
    "\n",
    "#     # logistic regression\n",
    "#     x_regression = np.linspace(30, 100, 70) # De 30 a 100, 70 valores equiespaciados\n",
    "#     y_regression = (-x_regression*W[0] - W[2])/W[1]\n",
    "\n",
    "#     # plots\n",
    "#     plt.scatter(admitted['X1'], admitted['X2'], s=10, label='Admitted')\n",
    "#     plt.scatter(not_admitted['X1'], not_admitted['X2'], s=10, label='Not Admitted')\n",
    "#     plt.plot(x_regression, y_regression, '-', color='green', label='Regression')\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
