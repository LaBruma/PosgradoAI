{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de módulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo el módulo de numpy\n",
    "#-------------------------\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#%matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levantar datos de disco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  CASO 1 entrada / 1 salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de la clase para levantar (y dividir) los datos\n",
    "# NOTA: Para un solo dato de entrada y una sola salida\n",
    "#===========================================================\n",
    "class Data(object):\n",
    "    def __init__(self, path):\n",
    "        self.dataset = self._build_dataset(path)\n",
    "        \n",
    "    def _build_dataset(self, path):\n",
    "        structure = [('X', np.float),\n",
    "                     ('y', np.float)]\n",
    "\n",
    "        with open(path, encoding=\"utf8\") as data_csv:\n",
    "\n",
    "            data_gen = ((float(line.split(',')[0]), float(line.split(',')[1])) # add here + 10 in second value\n",
    "                        for i, line in enumerate(data_csv) if i != 0)\n",
    "            embeddings = np.fromiter(data_gen, structure)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def split(self, percentage): # 0.8\n",
    "        X = self.dataset['X']\n",
    "        y = self.dataset['y']\n",
    "\n",
    "        # X.shape[0] -> 10 (filas)\n",
    "\n",
    "        permuted_idxs = np.random.permutation(X.shape[0])\n",
    "\n",
    "        train_idxs = permuted_idxs[0:int(percentage * X.shape[0])]\n",
    "\n",
    "        test_idxs = permuted_idxs[int(percentage * X.shape[0]): X.shape[0]]\n",
    "                    #[9,0]\n",
    "\n",
    "        X_train = X[train_idxs]\n",
    "        X_test = X[test_idxs]\n",
    "\n",
    "        y_train = y[train_idxs]\n",
    "        y_test = y[test_idxs]\n",
    "\n",
    "        return X_train, X_test, y_train, y_test  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CASO 2 entradas / 1 salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de la clase para levantar (y dividir) los datos\n",
    "# NOTA: Para dos datos de entrada y una sola salida\n",
    "#===========================================================\n",
    "class Data(object):\n",
    "    def __init__(self, path):\n",
    "        self.dataset = self._build_dataset(path)\n",
    "    \n",
    "    def _build_dataset(self, path):\n",
    "        # Armo una estructura de datos para guardarlos ahí\n",
    "        #-------------------------------------------------\n",
    "        structure = [('X1', np.float),\n",
    "                     ('X2', np.float),\n",
    "                     ('y', np.int)]\n",
    "        \n",
    "        # Abro el archivo lo recorro llenando la estructura creada línea a línea\n",
    "        #-----------------------------------------------------------------------\n",
    "        with open(path, encoding=\"utf8\") as data_csv:\n",
    "            # A la estructura (data_gen) hay que armarla según lo que figure en el archivo\n",
    "            data_gen = ((float(line.split(',')[0]), float(line.split(',')[1]), np.int(line.split(',')[2])) # add here + 10 in second value\n",
    "                        for i, line in enumerate(data_csv) if i != 0)\n",
    "            embeddings = np.fromiter(data_gen, structure)\n",
    "\n",
    "        return embeddings\n",
    "    \n",
    "    # Separo los los datos (train y test)\n",
    "    #------------------------------------\n",
    "    def split(self, percentage): # 0.8 en general\n",
    "        X = self.dataset[['X1', 'X2']]\n",
    "        y = self.dataset['y']\n",
    "        \n",
    "        # Permutamos los datos de entrada (asumimos que vienen en columna)\n",
    "        # Es decir, cada fila sería una ocurrencia distinta\n",
    "        #-----------------------------------------------------------------\n",
    "        permuted_idxs = np.random.permutation(len(X)) #len me da la dimensión mayor\n",
    "        \n",
    "        # Separamos en Train (80%) y Test (20%)\n",
    "        #--------------------------------------\n",
    "        train_idxs = permuted_idxs[0:int(percentage * X.shape[0])]\n",
    "        test_idxs = permuted_idxs[int(percentage * X.shape[0]): X.shape[0]]\n",
    "\n",
    "        X_train = X[train_idxs]\n",
    "        X_test = X[test_idxs]\n",
    "\n",
    "        y_train = y[train_idxs]\n",
    "        y_test = y[test_idxs]\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "        # Separamos en Train (70%), Test (20%) y Validation (10%)\n",
    "        #--------------------------------------------------------\n",
    "        #... train_idxs = permuted_idxs[0:int(percentage * X1.shape[0])]\n",
    "#         test_idxs = permuted_idxs[int(percentage * X1.shape[0]):int(0.9 * X1.shape[0])]\n",
    "#         valid_idxs = permuted_idxs[int(0.9 * X1.shape[0]):X1.shape[0]]\n",
    "\n",
    "#         X_train = np.vstack((X1[train_idxs],X2[train_idxs]))\n",
    "#         X_test = np.vstack((X1[test_idxs],X2[test_idxs]))\n",
    "#         X_valid = np.vstack((X1[valid_idxs],X2[valid_idxs]))\n",
    "\n",
    "#         y_train = y[train_idxs]\n",
    "#         y_test = y[test_idxs]\n",
    "#         y_valid = y[valid_idxs]\n",
    "        \n",
    "#         return X_train, X_test, X_valid, y_train, y_test, y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que realiza prepara los datos e itera llamando a k_means_loop (la que hace las cuentas)\n",
    "#------------------------------------------------------------------------------------------------\n",
    "def k_means(X, n_clusters):\n",
    "    # Armamos una matriz identidad con tamaño cantidad de centroides por cantidad de columnas de X\n",
    "    centroids = np.eye(n_clusters, X.shape[1])\n",
    "    print(centroids)\n",
    "    # Para la cantidad de iteraciones indicadas\n",
    "    for i in range(MAX_ITERATIONS):\n",
    "        print(\"Iteration # {}\".format(i))\n",
    "        # Corremos el algoritmo de k-means\n",
    "        centroids, cluster_ids = k_means_loop(X, centroids)\n",
    "        print(centroids)\n",
    "    return centroids, cluster_ids\n",
    "\n",
    "# Este es el loop que realiza los cálculos\n",
    "def k_means_loop(X, centroids):\n",
    "    # Encontramos etiquetas para las filas en X basado en los valores de los centroides\n",
    "    expanded_centroids = centroids[:, None]\n",
    "    distances = np.sqrt(np.sum((expanded_centroids - X) ** 2, axis=2))  # Calculo las distancias (norma 2) de los centroides a cada vector de X\n",
    "    arg_min = np.argmin(distances, axis=0) # Devuelve los índices de los vectores con menor distancia a los centroides\n",
    "    \n",
    "    # Recalcular los centroides\n",
    "    for i in range(centroids.shape[0]):\n",
    "        centroids[i] = np.mean(X[arg_min == i, :], axis=0)\n",
    "    \n",
    "    return centroids, arg_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA (por las dudas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-d456a3af43b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Ahora comparamos contra la función embebida de PCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Ahora comparamos contra la función embebida de PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "x_std = StandardScaler(with_std=False).fit_transform(x)\n",
    "pca.fit_transform(x_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresiones lineales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase base de la que heredan las que vayamos implementando\n",
    "#-----------------------------------------------------------\n",
    "# Es conveniente tener una clase base de la que vayan heredando las demás. Siempre habrá un método fit\n",
    "# y un método predict. Pero en esta clase base puede haber definiciones de atributos comunes a todas\n",
    "#===========================================================\n",
    "class BaseModel(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        return NotImplemented\n",
    "\n",
    "    def predict(self, X):\n",
    "        return NotImplemented\n",
    "\n",
    "\n",
    "class ConstantModel(BaseModel):\n",
    "    # El modelo constante solo saca la media de los datos y devuelve ese valor\n",
    "    # Es útil para comparar. Ningún modelo debería ser peor que este.\n",
    "    #-------------------------------------------------------------------------\n",
    "    def fit(self, X, Y):\n",
    "        W = Y.mean()\n",
    "        self.model = W\n",
    "\n",
    "    def predict(self, X):\n",
    "        # La \"predicción\" consiste en devolver la media para todos los valores\n",
    "        return np.ones(len(X)) * self.model\n",
    "\n",
    "# Modelo de la regresión lineal\n",
    "#==============================\n",
    "class LinearRegression(BaseModel):\n",
    "    # Este modelo de regresión lineal ajusta únicamente la pendiente, no contempla la ordenada al origen\n",
    "    def fit(self, X, y):\n",
    "        # Verificamos si X es un vector o una matriz\n",
    "        if len(X.shape) == 1:\n",
    "            # Esta es una manera de escribir la pseudo-inversa (X'.X)^(-1).X'.y\n",
    "            W = X.T.dot(y) / X.T.dot(X)\n",
    "        else:\n",
    "            # Y esta es la manera con matrices\n",
    "            W = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "        self.model = W\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X.dot(self.model)\n",
    "    \n",
    "# Modelo que incluye la ordenada al origen (b)\n",
    "# ============================================\n",
    "class LinearRegressionWithB(BaseModel):\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # En el caso de ajustar con ordenada al origen le agregamos la columna de b con unos\n",
    "        # (Le agrega la fila abajo y luego traspongo --> Vectores columna)\n",
    "        X_expanded = np.vstack((X, np.ones(len(X)))).T\n",
    "        W = np.linalg.inv(X_expanded.T.dot(X_expanded)).dot(X_expanded.T).dot(y)\n",
    "        self.model = W\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_expanded = np.vstack((X, np.ones(len(X)))).T\n",
    "        return X_expanded.dot(self.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradientes descendentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, lr=0.01, amt_epochs=100):\n",
    "    \"\"\"\n",
    "    lr: learning rate\n",
    "    amt_epochs: cantidad de iteraciones\n",
    "    \n",
    "    shapes: \n",
    "        X_t: nxm\n",
    "        Y_y: nx1\n",
    "        W: mx1\n",
    "    \"\"\"\n",
    "    n = X_train.shape[0]\n",
    "    m = X_train.shape[1]\n",
    "    # print('X.shape:{}x{}\\n'.format(n,m))\n",
    "        \n",
    "    # Inicializamos los pesos\n",
    "    W = np.random.randn(m).reshape(m,1)\n",
    "    print('W_inicial_{}'.format(W.reshape(-1)))\n",
    "    \n",
    "    for i in range(amt_epochs):\n",
    "        # Calculo la estimación\n",
    "        #y_hat=X_train*W\n",
    "        y_hat=np.matmul(X_train,W)\n",
    "        \n",
    "        # Calculo el error\n",
    "        error=y_train-y_hat\n",
    "        \n",
    "        # Calculo el gradiente\n",
    "        grad_sum = np.sum(error*X_train,axis=0)\n",
    "        grad_mul =-2/n*grad_sum  #1xm\n",
    "        gradient = np.transpose(grad_mul).reshape(-1,1) #mx1\n",
    "        \n",
    "        # Actualizo el valor\n",
    "        W = W - (lr*gradient)\n",
    "    \n",
    "    return W\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(X_train, y_train, lr=0.01, amt_epochs=100):\n",
    "    \"\"\"\n",
    "    lr: learning rate\n",
    "    amt_epochs: cantidad de iteraciones\n",
    "    \n",
    "    shapes: \n",
    "        X_t: nxm\n",
    "        Y_y: nx1\n",
    "        W: mx1\n",
    "    \"\"\"\n",
    "    n = X_train.shape[0]\n",
    "    m = X_train.shape[1]\n",
    "    # print('X.shape:{}x{}\\n'.format(n,m))\n",
    "        \n",
    "    # Inicializamos los pesos\n",
    "    W = np.random.randn(m).reshape(m,1)\n",
    "    print('W_inicial_{}'.format(W.reshape(-1)))\n",
    "    \n",
    "    for i in range(amt_epochs):\n",
    "        idx=np.random.permutation(X_train.shape[0])\n",
    "        X_train = X_train[idx]\n",
    "        y_train = y_train[idx]\n",
    "        \n",
    "        for j in range(n):\n",
    "        \n",
    "            # Calculo la estimación\n",
    "            #y_hat=X_train*W\n",
    "            y_hat=np.matmul(X_train[j].reshape(1,-1),W)\n",
    "\n",
    "            # Calculo el error\n",
    "            error=y_train[j]-y_hat\n",
    "\n",
    "            # Calculo el gradiente\n",
    "            grad_sum = error*X_train[j]\n",
    "            grad_mul =-2/n*grad_sum  #1xm\n",
    "            gradient = np.transpose(grad_mul).reshape(-1,1) #mx1\n",
    "\n",
    "            # Actualizo el valor\n",
    "            W = W - (lr*gradient)\n",
    "    \n",
    "    return W\n",
    "\n",
    "def mini_batch_gradient_descent(X_train, y_train, lr=0.01, amt_epochs=100):\n",
    "    \"\"\"\n",
    "    shapes:\n",
    "        X_t = nxm\n",
    "        y_t = nx1\n",
    "        W = mx1\n",
    "    \"\"\"\n",
    "    b = 16\n",
    "    n = X_train.shape[0]\n",
    "    m = X_train.shape[1]\n",
    "\n",
    "    # initialize random weights\n",
    "    W = np.random.randn(m).reshape(m, 1)\n",
    "\n",
    "    for i in range(amt_epochs):\n",
    "        idx = np.random.permutation(X_train.shape[0])\n",
    "        X_train = X_train[idx]\n",
    "        y_train = y_train[idx]\n",
    "\n",
    "        batch_size = int(len(X_train) / b)\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            end = i + batch_size if i + batch_size <= len(X_train) else len(X_train)\n",
    "            batch_X = X_train[i: end]\n",
    "            batch_y = y_train[i: end]\n",
    "\n",
    "            prediction = np.matmul(batch_X, W)  # nx1\n",
    "            error = batch_y - prediction  # nx1\n",
    "\n",
    "            grad_sum = np.sum(error * batch_X, axis=0)\n",
    "            grad_mul = -2/n * grad_sum  # 1xm\n",
    "            gradient = np.transpose(grad_mul).reshape(-1, 1)  # mx1\n",
    "\n",
    "            W = W - (lr * gradient)\n",
    "\n",
    "    return W\n",
    "\n",
    "\n",
    "def mini_batch_logistic_regression(X_train, y_train, lr=0.01, amt_epochs=100):\n",
    "    \"\"\"\n",
    "    shapes:\n",
    "        X_t = nxm\n",
    "        y_t = nx1\n",
    "        W = mx1\n",
    "    \"\"\"\n",
    "    b = 16\n",
    "    n = X_train.shape[0]\n",
    "    m = X_train.shape[1]\n",
    "\n",
    "    # initialize random weights\n",
    "    W = np.random.randn(m).reshape(m, 1)\n",
    "\n",
    "    for i in range(amt_epochs):\n",
    "        idx = np.random.permutation(X_train.shape[0])\n",
    "        X_train = X_train[idx]\n",
    "        y_train = y_train[idx]\n",
    "\n",
    "        batch_size = int( len(X_train) / b)\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            end = i + batch_size if i + batch_size <= len(X_train) else len(X_train)\n",
    "            batch_X = X_train[i: end]\n",
    "            batch_y = y_train[i: end]\n",
    "            prediction = 1/(1+np.exp(-np.matmul(batch_X, W))) #Ojo que no es la predicción posta!!\n",
    "            error = batch_y - prediction  # nx1\n",
    "\n",
    "            grad_sum = np.sum(error * batch_X, axis=0)\n",
    "            grad_mul = -2/b * grad_sum  # 1xm\n",
    "            gradient = np.transpose(grad_mul).reshape(-1, 1)  # mx1\n",
    "\n",
    "            W = W - (lr * gradient)\n",
    "\n",
    "    return W\n",
    "\n",
    "def sin_fitting_example():\n",
    "    # y = sin(x)\n",
    "    amt_points = 36\n",
    "    x = np.linspace(0, 360, num=amt_points)\n",
    "    y = np.sin(x * np.pi / 180.)\n",
    "    noise = np.random.normal(0, .1, y.shape)\n",
    "    noisy_y = y + noise\n",
    "\n",
    "    X_train = x\n",
    "    y_train = noisy_y\n",
    "\n",
    "    regression = LinearRegression()\n",
    "\n",
    "    # linear\n",
    "    X_linear = np.vstack((X_train, np.ones(len(X_train)))).T\n",
    "    regression.fit(X_linear, y_train.reshape(-1, 1))\n",
    "    W_linear = regression.model\n",
    "    y_linear = W_linear[0]*x + W_linear[1]\n",
    "\n",
    "    # quadratic\n",
    "    X_quadratic = np.vstack((np.power(X_train, 2), X_train, np.ones(len(X_train)))).T\n",
    "    regression.fit(X_quadratic, y_train.reshape(-1, 1))\n",
    "    W_quadratic = regression.model\n",
    "    y_quadratic = W_quadratic[0] * np.power(x, 2) + W_quadratic[1] * x + W_quadratic[2]\n",
    "\n",
    "    # cubic\n",
    "    X_cubic = np.vstack((np.power(X_train, 3), np.power(X_train, 2), X_train, np.ones(len(X_train)))).T\n",
    "    regression.fit(X_cubic, y_train.reshape(-1, 1))\n",
    "    W_cubic = regression.model\n",
    "    y_cubic = W_cubic[0] * np.power(x, 3) + W_cubic[1] * np.power(x, 2) + W_cubic[2] * x + W_cubic[3]\n",
    "\n",
    "    # X10\n",
    "    X_10 = np.vstack((np.power(X_train, 10), np.power(X_train, 9), np.power(X_train, 8),\n",
    "                      np.power(X_train, 7), np.power(X_train, 6), np.power(X_train, 5),\n",
    "                      np.power(X_train, 4), np.power(X_train, 3), np.power(X_train, 2),\n",
    "                      X_train, np.ones(len(X_train)))).T\n",
    "    regression.fit(X_10, y_train.reshape(-1, 1))\n",
    "    W_10 = regression.model\n",
    "    y_10 = W_10[0] * np.power(x, 10) + W_10[1] * np.power(x, 9) + W_10[2] * np.power(x, 8) + \\\n",
    "           W_10[3] * np.power(x, 7) + W_10[4] * np.power(x, 6) + W_10[5] * np.power(x, 5) + \\\n",
    "           W_10[6] * np.power(x, 4) + W_10[7] * np.power(x, 3) + W_10[8] * np.power(x, 2) + \\\n",
    "           W_10[9] * x + W_10[10]\n",
    "\n",
    "    # PLOTS\n",
    "    plt.figure()\n",
    "    plt.subplot(1, 1, 1)\n",
    "    plt.gca().set_title('Sin(x) - Fitting curves')\n",
    "\n",
    "    # original\n",
    "    plt.plot(x, noisy_y, 'o')\n",
    "\n",
    "    # linear\n",
    "    plt.plot(x, y_linear, '-')\n",
    "\n",
    "    # quadratic\n",
    "    plt.plot(x, y_quadratic, '-')\n",
    "\n",
    "    # cubic\n",
    "    plt.plot(x, y_cubic, '-')\n",
    "\n",
    "    # 10 power\n",
    "    plt.plot(x, y_10, '-')\n",
    "\n",
    "    plt.legend(['noisy signal', 'linear', 'quadratic', 'cubic', '10th power'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_folds(X_train, y_train, k=5):\n",
    "    l_regression = LinearRegression()\n",
    "    error = MSE()\n",
    "\n",
    "    chunk_size = int(len(X_train) / k)\n",
    "    mse_list = []\n",
    "    for i in range(0, len(X_train), chunk_size):\n",
    "        end = i + chunk_size if i + chunk_size <= len(X_train) else len(X_train)\n",
    "        new_X_valid = X_train[i: end]\n",
    "        new_y_valid = y_train[i: end]\n",
    "        new_X_train = np.concatenate([X_train[: i], X_train[end:]])\n",
    "        new_y_train = np.concatenate([y_train[: i], y_train[end:]])\n",
    "\n",
    "        l_regression.fit(new_X_train, new_y_train)\n",
    "        prediction = l_regression.predict(new_X_valid)\n",
    "        mse_list.append(error(new_y_valid, prediction))\n",
    "\n",
    "    mean_MSE = np.mean(mse_list)\n",
    "\n",
    "    return mean_MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clases de métricas\n",
    "#===================\n",
    "\n",
    "# Clase madre\n",
    "class Metric(object):\n",
    "    def __call__(self, target, prediction):\n",
    "        return NotImplemented\n",
    "\n",
    "# Por ahora solo esta --> Error cuadrático medio\n",
    "class MSE(Metric):\n",
    "    def __init__(self):\n",
    "        Metric.__init__(self)\n",
    "        \n",
    "    def __call__(self, target, prediction):\n",
    "        n = target.size\n",
    "        return np.sum((target - prediction) ** 2) / n\n",
    "\n",
    "class Precision(Metric):\n",
    "    def __init__(self):\n",
    "        Metric.__init__(self)\n",
    "        \n",
    "    def __call__(self, truth, prediction):\n",
    "        \n",
    "        # Encontramos los True Positive\n",
    "        true_pos_mask = (prediction == 1) & (truth == 1)\n",
    "        true_pos = true_pos_mask.sum()\n",
    "        \n",
    "        # Encontramos los False Positive\n",
    "        false_pos_mask = (prediction == 1) & (truth == 0)\n",
    "        false_pos = false_pos_mask.sum()\n",
    "        \n",
    "        return true_pos / (true_pos + false_pos)\n",
    "\n",
    "class Recall (Metric):\n",
    "    def __init__(self):\n",
    "        Metric.__init__(self)\n",
    "        \n",
    "    def __call__(self, truth, prediction):\n",
    "        \n",
    "        # Encontramos los True Positive\n",
    "        true_pos_mask = (prediction == 1) & (truth == 1)\n",
    "        true_pos = true_pos_mask.sum()\n",
    "        \n",
    "        # Encontramos los False Negative\n",
    "        false_neg_mask = (prediction == 0) & (truth == 1)\n",
    "        false_neg = false_neg_mask.sum()\n",
    "        \n",
    "        return true_pos / (true_pos + false_neg)\n",
    "        \n",
    "class Accuracy (Metric):\n",
    "    def __init__(self):\n",
    "        Metric.__init__(self)\n",
    "        \n",
    "    def __call__(self, truth, prediction):\n",
    "        \n",
    "        # Encontramos los True Positive\n",
    "        true_pos_mask = (prediction == 1) & (truth == 1)\n",
    "        true_pos = true_pos_mask.sum()\n",
    "        \n",
    "        # Encontramos los False Positive\n",
    "        false_pos_mask = (prediction == 1) & (truth == 0)\n",
    "        false_pos = false_pos_mask.sum()\n",
    "        \n",
    "        # Encontramos los True Negative\n",
    "        true_neg_mask = (prediction == 0) & (truth == 0)\n",
    "        true_neg = true_neg_mask.sum() \n",
    "    \n",
    "        # Encontramos los False Negative\n",
    "        false_neg_mask = (prediction == 0) & (truth == 1)\n",
    "        false_neg = false_neg_mask.sum()\n",
    "    \n",
    "        return (true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examen Final - Intro a AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Punto 2\n",
    "\n",
    "- a) Obtener el dataset desde el siguiente link. La primera columna representa los datos de entrada y la segunda columna representa los datos de salida.\n",
    "\n",
    "- b) Levantar el dataset en un arreglo de Numpy. \n",
    "\n",
    "- c) Graficar el dataset de manera tal que sea posible visualizar la nube de puntos.\n",
    "\n",
    "- d) Partir el dataset en train (80%) y test (20%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Levantamos en un arreglo Numpy los datos\n",
    "dataset = Data('../clase_8_dataset.csv')\n",
    "# c) Para mostrarlos todos hago un \"split mentiroso\" al 100%. Todos los datos quedan en Train\n",
    "X_train, X_test, y_train, y_test = dataset.split(1)\n",
    "# Graficamos...\n",
    "plt.scatter(X_train, y_train, s=10, label='Dataset')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# d) Ahora sí particionamos el dataset en train (80%) y test (20%)\n",
    "X_train, X_test, y_train, y_test = dataset.split(0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Punto 3\n",
    "\n",
    "Utilizar regresión polinómica para hacer “fit” sobre la nube de puntos del train. Para este ejercicio, se desea utilizar la fórmula cerrada de la optimización polinómica. El modelo es de la forma y = [Wn … W0] * [X^n    X^(n-1)    …    1]. \n",
    "\n",
    "- a) Para n = 1 (modelo lineal con ordenada al origen), hacer un fit del modelo utilizando K-FOLDS. Para K-FOLDS partir el train dataset en 5 partes iguales, utilizar 4/5 para entrenar y 1/5 para validar. Informar el mejor modelo obtenido y el criterio utilizado para elegir dicho modelo (dejar comentarios en el código).\n",
    "\n",
    "- b) Repetir el punto (a), para n = {2,3,4}. Computar el error de validación y test del mejor modelo para cada n.\n",
    "\n",
    "- c) Elegir el polinomio que hace mejor fit sobre la nube de puntos y explicar el criterio seleccionado (dejar comentarios en el código). \n",
    "\n",
    "- d) Graficar el polinomio obtenido y el dataset de test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79, 2)\n",
      "(79, 1)\n",
      "(79, 1)\n",
      "(79, 2)\n",
      "(79, 1)\n",
      "(79, 1)\n",
      "(79, 2)\n",
      "(79, 1)\n",
      "(79, 1)\n",
      "(79, 2)\n",
      "(79, 1)\n",
      "(79, 1)\n",
      "(79, 2)\n",
      "(79, 1)\n",
      "(79, 1)\n",
      "(4, 2)\n",
      "(4, 1)\n",
      "(4, 1)\n",
      "Error de ajuste n=1: 810.5364955247574\n",
      "(79, 3)\n",
      "(79,)\n",
      "(79,)\n",
      "(79, 3)\n",
      "(79,)\n",
      "(79,)\n",
      "(79, 3)\n",
      "(79,)\n",
      "(79,)\n",
      "(79, 3)\n",
      "(79,)\n",
      "(79,)\n",
      "(79, 3)\n",
      "(79,)\n",
      "(79,)\n",
      "(4, 3)\n",
      "(4,)\n",
      "(4,)\n",
      "Error de ajuste n=2: 88.54659396434538\n",
      "(79, 4)\n",
      "(79,)\n",
      "(79,)\n",
      "(79, 4)\n",
      "(79,)\n",
      "(79,)\n",
      "(79, 4)\n",
      "(79,)\n",
      "(79,)\n",
      "(79, 4)\n",
      "(79,)\n",
      "(79,)\n",
      "(79, 4)\n",
      "(79,)\n",
      "(79,)\n",
      "(4, 4)\n",
      "(4,)\n",
      "(4,)\n",
      "Error de ajuste n=3: 10.702641143079703\n",
      "(79, 5)\n",
      "(79,)\n",
      "(79,)\n",
      "(79, 5)\n",
      "(79,)\n",
      "(79,)\n",
      "(79, 5)\n",
      "(79,)\n",
      "(79,)\n",
      "(79, 5)\n",
      "(79,)\n",
      "(79,)\n",
      "(79, 5)\n",
      "(79,)\n",
      "(79,)\n",
      "(4, 5)\n",
      "(4,)\n",
      "(4,)\n",
      "Error de ajuste n=4: 10.925876144103503\n"
     ]
    }
   ],
   "source": [
    "# a) K-folds para el caso lineal\n",
    "# n=1: Caso Lineal\n",
    "X_linear = np.vstack((X_train, np.ones(len(X_train)))).T\n",
    "mean_MSE = k_folds(X_linear, y_train.reshape(-1, 1), k=5)\n",
    "print(\"Error de ajuste n=1: {}\".format(mean_MSE))\n",
    "\n",
    "# b) K-folds para los demás órdenes\n",
    "# n=2: Caso cuadratico\n",
    "X_quadratic = np.vstack((np.power(X_train, 2), X_train, np.ones(len(X_train)))).T\n",
    "mean_MSE = k_folds(X_quadratic, y_train, k=5)\n",
    "print(\"Error de ajuste n=2: {}\".format(mean_MSE))\n",
    "\n",
    "# n=3: Caso cúbico\n",
    "X_cubic = np.vstack((np.power(X_train, 3), np.power(X_train, 2), X_train, np.ones(len(X_train)))).T\n",
    "mean_MSE = k_folds(X_cubic, y_train, k=5)\n",
    "print(\"Error de ajuste n=3: {}\".format(mean_MSE))\n",
    "\n",
    "# n=4: Caso poly 4\n",
    "X_4 = np.vstack((np.power(X_train, 4), np.power(X_train, 3), np.power(X_train, 2), X_train, np.ones(len(X_train)))).T\n",
    "mean_MSE = k_folds(X_4, y_train, k=5)\n",
    "print(\"Error de ajuste n=4: {}\".format(mean_MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Conclusión:: \n",
    "De estos resultados podemos ver que el error de ajuste para n=3 es el más pequeño (inclusive mejor que n=4). Tomamos entonces n=3 para seguir. Todo esto está basado en la técnica de K-folds que va partiendo el train dataset en las partes que se le indique (en nuestro caso k=5) y encontrando un promedio de los errores cuadráticos medios entrenando sobre 4/5 de las particiones y validando sobre 1/5, para todas las combinaciones.\n",
    "\n",
    "En resumen, como dijimos, tomamos n=3 para seguir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d) Graficamos el polinomio y el dataset de test\n",
    "# n=3: Caso cúbico\n",
    "X_cubic = np.vstack((np.power(X_train, 3), np.power(X_train, 2), X_train, np.ones(len(X_train)))).T\n",
    "\n",
    "x = np.linspace(-400, 400, num=800)\n",
    "\n",
    "regression.fit(X_cubic, y_train.reshape(-1, 1))\n",
    "W_cubic = regression.model\n",
    "y_cubic = W_cubic[0] * np.power(x, 3) + W_cubic[1] * np.power(x, 2) + W_cubic[2] * x + W_cubic[3]\n",
    "\n",
    "# Graficamos\n",
    "plt.scatter(X_test, y_test, s=10, label='Test data')\n",
    "plt.plot(x, y_cubic, 'r-')\n",
    "#plt.legend(['noisy signal', 'linear', 'quadratic', 'cubic', '10th power'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================\n",
    "# Código que tenía para pruebas\n",
    "#==============================\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     dataset = Data('../income.data.csv')\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = dataset.split(0.8)\n",
    "\n",
    "#     linear_regression = LinearRegression()\n",
    "#     linear_regression.fit(X_train, y_train)\n",
    "#     lr_y_hat = linear_regression.predict(X_test)\n",
    "\n",
    "#     linear_regression_b = LinearRegressionWithB()\n",
    "#     linear_regression_b.fit(X_train, y_train)\n",
    "#     lrb_y_hat = linear_regression_b.predict(X_test)\n",
    "\n",
    "#     constant_model = ConstantModel()\n",
    "#     constant_model.fit(X_train, y_train)\n",
    "#     ct_y_hat = constant_model.predict(X_test)\n",
    "\n",
    "#     mse = MSE()\n",
    "#     lr_mse = mse(y_test, lr_y_hat)\n",
    "#     lrb_mse = mse(y_test, lrb_y_hat)\n",
    "#     ct_mse = mse(y_test, ct_y_hat)\n",
    "\n",
    "#     x_plot = np.linspace(0, 10, 10)\n",
    "#     lr_y_plot = linear_regression.model * x_plot\n",
    "#     lrb_y_plot = linear_regression_b.model[0] * x_plot + linear_regression_b.model[1]\n",
    "\n",
    "#     # gradient descent\n",
    "#     print('\\n\\n\\nGRADIENT DESCENT VS LINEAR REGRESSION')\n",
    "#     lr_1 = 0.001\n",
    "#     amt_epochs_1 = 1000\n",
    "#     start = time.time()\n",
    "#     W_manual = gradient_descent(X_train.reshape(-1, 1), y_train.reshape(-1, 1), lr=lr_1, amt_epochs=amt_epochs_1)\n",
    "#     time_1 = time.time() - start\n",
    "#     W_real = linear_regression.model\n",
    "#     print('W_manual:  {}\\nW_real:    {}\\nManual time [s]: {}'.format(W_manual.reshape(-1), W_real, time_1))\n",
    "\n",
    "#     # gradient descent\n",
    "#     print('\\n\\n\\nGRADIENT DESCENT VS LINEAR REGRESSION WITH B')\n",
    "#     X_expanded = np.vstack((X_train, np.ones(len(X_train)))).T\n",
    "#     lr_2 = 0.001\n",
    "#     amt_epochs_2 = 100000\n",
    "#     start = time.time()\n",
    "#     W_manual = gradient_descent(X_expanded, y_train.reshape(-1, 1), lr=lr_2, amt_epochs=amt_epochs_2)\n",
    "#     time_2 = time.time() - start\n",
    "#     W_real = linear_regression_b.model\n",
    "#     print('W_manual:  {}\\nW_real:    {}\\nManual time [s]: {}'.\n",
    "#           format(W_manual.reshape(-1), W_real, time_2))\n",
    "\n",
    "#     # gradient descent\n",
    "#     print('\\n\\n\\nSTOCHASTIC GRADIENT DESCENT VS LINEAR REGRESSION WITH B')\n",
    "#     X_expanded = np.vstack((X_train, np.ones(len(X_train)))).T\n",
    "#     lr_3 = 0.05\n",
    "#     amt_epochs_3 = 1000\n",
    "#     start = time.time()\n",
    "#     W_manual = stochastic_gradient_descent(X_expanded, y_train.reshape(-1, 1), lr=lr_3, amt_epochs=amt_epochs_3)\n",
    "#     time_3 = time.time() - start\n",
    "#     W_real = linear_regression_b.model\n",
    "#     print('W_manual:  {}\\nW_real:    {}\\nManual time [s]: {}'.\n",
    "#           format(W_manual.reshape(-1), W_real, time_3))\n",
    "\n",
    "#     # gradient descent\n",
    "#     print('\\n\\n\\nMINI BATCH GRADIENT DESCENT VS LINEAR REGRESSION WITH B')\n",
    "#     X_expanded = np.vstack((X_train, np.ones(len(X_train)))).T\n",
    "#     lr_4 = 0.05\n",
    "#     amt_epochs_4 = 10000\n",
    "#     start = time.time()\n",
    "#     W_manual = mini_batch_gradient_descent(X_expanded, y_train.reshape(-1, 1), lr=lr_4, amt_epochs=amt_epochs_4)\n",
    "#     time_4 = time.time() - start\n",
    "#     W_real = linear_regression_b.model\n",
    "#     print('W_manual:  {}\\nW_real:    {}\\nManual time [s]: {}'.\n",
    "#           format(W_manual.reshape(-1), W_real, time_4))\n",
    "\n",
    "#     # PLOTS\n",
    "#     plt.figure()\n",
    "#     x_plot = np.linspace(1, 4, 4)\n",
    "#     legend = ['GD', 'GD(B)', 'S-GD(B)', 'MB-GD(B)']\n",
    "\n",
    "#     plt.subplot(1, 3, 1)\n",
    "#     plt.gca().set_title('Learning Rate')\n",
    "#     y_plot = [lr_1, lr_2, lr_3, lr_4]\n",
    "#     plt.plot(x_plot[0], y_plot[0], 'o', x_plot[1], y_plot[1], 'o', x_plot[2], y_plot[2], 'o',\n",
    "#              x_plot[3], y_plot[3], 'o')\n",
    "#     plt.legend(legend)\n",
    "#     for x, y in zip(x_plot, y_plot):\n",
    "#         plt.text(x, y, str(y))\n",
    "\n",
    "#     plt.subplot(1, 3, 2)\n",
    "#     plt.gca().set_title('Epochs')\n",
    "#     y_plot = [amt_epochs_1, amt_epochs_2, amt_epochs_3, amt_epochs_4]\n",
    "#     plt.plot(x_plot[0], y_plot[0], 'o', x_plot[1], y_plot[1], 'o', x_plot[2], y_plot[2], 'o',\n",
    "#              x_plot[3], y_plot[3], 'o')\n",
    "#     plt.legend(legend)\n",
    "#     for x, y in zip(x_plot, y_plot):\n",
    "#         plt.text(x, y, str(y))\n",
    "\n",
    "#     plt.subplot(1, 3, 3)\n",
    "#     plt.gca().set_title('Time')\n",
    "#     y_plot = [time_1, time_2, time_3, time_4]\n",
    "#     plt.plot(x_plot[0], y_plot[0], 'o', x_plot[1], y_plot[1], 'o', x_plot[2], y_plot[2], 'o',\n",
    "#              x_plot[3], y_plot[3], 'o')\n",
    "#     plt.legend(legend)\n",
    "#     for x, y in zip(x_plot, y_plot):\n",
    "#         plt.text(x, y, str(y))\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "#     # sin fitting example\n",
    "#     sin_fitting_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded\n",
      "Dataset split\n",
      "Training\n",
      "W: [  0.12245323   0.11537612 -13.68000208]\n",
      "Time [s]: 50.46697926521301\n"
     ]
    }
   ],
   "source": [
    "#==============================\n",
    "# Código que tenía para pruebas\n",
    "#==============================\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     dataset = Data('../clase_6_dataset.txt')\n",
    "#     print('Dataset loaded')\n",
    "#     X_train, X_test, y_train, y_test = dataset.split(1)\n",
    "#     print('Dataset split')\n",
    "\n",
    "#     # gradient descent (es importante que tenga la columna de unos)\n",
    "#     X_expanded = np.vstack((X_train['X1'], X_train['X2'], np.ones(len(X_train)))).T\n",
    "#     lr = 0.001\n",
    "#     amt_epochs = 50000\n",
    "#     print('Training')\n",
    "#     start = time.time()\n",
    "#     W = mini_batch_logistic_regression(X_expanded, y_train.reshape(-1, 1), lr=lr,amt_epochs=amt_epochs)\n",
    "#     time = time.time() - start\n",
    "#     print('W: {}\\nTime [s]: {}'.format(W.reshape(-1), time))\n",
    "\n",
    "#     # PLOTS\n",
    "#     # filter out the applicants that got admitted\n",
    "#     admitted = X_train[y_train == 1]\n",
    "#     # filter out the applicants that didn't get admission\n",
    "#     not_admitted = X_train[y_train == 0]\n",
    "\n",
    "#     # logistic regression\n",
    "#     x_regression = np.linspace(30, 100, 70) # De 30 a 100, 70 valores equiespaciados\n",
    "#     y_regression = (-x_regression*W[0] - W[2])/W[1]\n",
    "\n",
    "#     # plots\n",
    "#     plt.scatter(admitted['X1'], admitted['X2'], s=10, label='Admitted')\n",
    "#     plt.scatter(not_admitted['X1'], not_admitted['X2'], s=10, label='Not Admitted')\n",
    "#     plt.plot(x_regression, y_regression, '-', color='green', label='Regression')\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
