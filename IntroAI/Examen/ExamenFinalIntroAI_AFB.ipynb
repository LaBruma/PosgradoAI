{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de módulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo el módulo de numpy\n",
    "#-------------------------\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levantar datos de disco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  CASO 1 entrada / 1 salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de la clase para levantar (y dividir) los datos\n",
    "# NOTA: Para un solo dato de entrada y una sola salida\n",
    "#===========================================================\n",
    "class Data(object):\n",
    "    def __init__(self, path):\n",
    "        self.dataset = self._build_dataset(path)\n",
    "        \n",
    "    def _build_dataset(self, path):\n",
    "        structure = [('X', np.float),\n",
    "                     ('y', np.float)]\n",
    "\n",
    "        with open(path, encoding=\"utf8\") as data_csv:\n",
    "\n",
    "            data_gen = ((float(line.split(',')[0]), float(line.split(',')[1])) # add here + 10 in second value\n",
    "                        for i, line in enumerate(data_csv) if i != 0)\n",
    "            embeddings = np.fromiter(data_gen, structure)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def split(self, percentage): # 0.8\n",
    "        X = self.dataset['X']\n",
    "        y = self.dataset['y']\n",
    "\n",
    "        # X.shape[0] -> 10 (filas)\n",
    "\n",
    "        permuted_idxs = np.random.permutation(X.shape[0])\n",
    "\n",
    "        train_idxs = permuted_idxs[0:int(percentage * X.shape[0])]\n",
    "\n",
    "        test_idxs = permuted_idxs[int(percentage * X.shape[0]): X.shape[0]]\n",
    "                    #[9,0]\n",
    "\n",
    "        X_train = X[train_idxs]\n",
    "        X_test = X[test_idxs]\n",
    "\n",
    "        y_train = y[train_idxs]\n",
    "        y_test = y[test_idxs]\n",
    "\n",
    "        return X_train, X_test, y_train, y_test  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CASO 2 entradas / 1 salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de la clase para levantar (y dividir) los datos\n",
    "# NOTA: Para dos datos de entrada y una sola salida\n",
    "#===========================================================\n",
    "class Data(object):\n",
    "    def __init__(self, path):\n",
    "        self.dataset = self._build_dataset(path)\n",
    "    \n",
    "    def _build_dataset(self, path):\n",
    "        # Armo una estructura de datos para guardarlos ahí\n",
    "        #-------------------------------------------------\n",
    "        structure = [('X1', np.float),\n",
    "                     ('X2', np.float),\n",
    "                     ('y', np.int)]\n",
    "        \n",
    "        # Abro el archivo lo recorro llenando la estructura creada línea a línea\n",
    "        #-----------------------------------------------------------------------\n",
    "        with open(path, encoding=\"utf8\") as data_csv:\n",
    "            # A la estructura (data_gen) hay que armarla según lo que figure en el archivo\n",
    "            data_gen = ((float(line.split(',')[0]), float(line.split(',')[1]), np.int(line.split(',')[2])) # add here + 10 in second value\n",
    "                        for i, line in enumerate(data_csv) if i != 0)\n",
    "            embeddings = np.fromiter(data_gen, structure)\n",
    "\n",
    "        return embeddings\n",
    "    \n",
    "    # Separo los los datos (train y test)\n",
    "    #------------------------------------\n",
    "    def split(self, percentage): # 0.8 en general\n",
    "        X = self.dataset[['X1', 'X2']]\n",
    "        y = self.dataset['y']\n",
    "        \n",
    "        # Permutamos los datos de entrada (asumimos que vienen en columna)\n",
    "        # Es decir, cada fila sería una ocurrencia distinta\n",
    "        #-----------------------------------------------------------------\n",
    "        permuted_idxs = np.random.permutation(len(X)) #len me da la dimensión mayor\n",
    "        \n",
    "        # Separamos en Train (80%) y Test (20%)\n",
    "        #--------------------------------------\n",
    "        train_idxs = permuted_idxs[0:int(percentage * X.shape[0])]\n",
    "        test_idxs = permuted_idxs[int(percentage * X.shape[0]): X.shape[0]]\n",
    "\n",
    "        X_train = X[train_idxs]\n",
    "        X_test = X[test_idxs]\n",
    "\n",
    "        y_train = y[train_idxs]\n",
    "        y_test = y[test_idxs]\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "        # Separamos en Train (70%), Test (20%) y Validation (10%)\n",
    "        #--------------------------------------------------------\n",
    "        #... train_idxs = permuted_idxs[0:int(percentage * X1.shape[0])]\n",
    "#         test_idxs = permuted_idxs[int(percentage * X1.shape[0]):int(0.9 * X1.shape[0])]\n",
    "#         valid_idxs = permuted_idxs[int(0.9 * X1.shape[0]):X1.shape[0]]\n",
    "\n",
    "#         X_train = np.vstack((X1[train_idxs],X2[train_idxs]))\n",
    "#         X_test = np.vstack((X1[test_idxs],X2[test_idxs]))\n",
    "#         X_valid = np.vstack((X1[valid_idxs],X2[valid_idxs]))\n",
    "\n",
    "#         y_train = y[train_idxs]\n",
    "#         y_test = y[test_idxs]\n",
    "#         y_valid = y[valid_idxs]\n",
    "        \n",
    "#         return X_train, X_test, X_valid, y_train, y_test, y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que realiza prepara los datos e itera llamando a k_means_loop (la que hace las cuentas)\n",
    "#------------------------------------------------------------------------------------------------\n",
    "def k_means(X, n_clusters):\n",
    "    # Armamos una matriz identidad con tamaño cantidad de centroides por cantidad de columnas de X\n",
    "    centroids = np.eye(n_clusters, X.shape[1])\n",
    "    print(centroids)\n",
    "    # Para la cantidad de iteraciones indicadas\n",
    "    for i in range(MAX_ITERATIONS):\n",
    "        print(\"Iteration # {}\".format(i))\n",
    "        # Corremos el algoritmo de k-means\n",
    "        centroids, cluster_ids = k_means_loop(X, centroids)\n",
    "        print(centroids)\n",
    "    return centroids, cluster_ids\n",
    "\n",
    "# Este es el loop que realiza los cálculos\n",
    "def k_means_loop(X, centroids):\n",
    "    # Encontramos etiquetas para las filas en X basado en los valores de los centroides\n",
    "    expanded_centroids = centroids[:, None]\n",
    "    distances = np.sqrt(np.sum((expanded_centroids - X) ** 2, axis=2))  # Calculo las distancias (norma 2) de los centroides a cada vector de X\n",
    "    arg_min = np.argmin(distances, axis=0) # Devuelve los índices de los vectores con menor distancia a los centroides\n",
    "    \n",
    "    # Recalcular los centroides\n",
    "    for i in range(centroids.shape[0]):\n",
    "        centroids[i] = np.mean(X[arg_min == i, :], axis=0)\n",
    "    \n",
    "    return centroids, arg_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA (por las dudas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-d456a3af43b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Ahora comparamos contra la función embebida de PCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Ahora comparamos contra la función embebida de PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "x_std = StandardScaler(with_std=False).fit_transform(x)\n",
    "pca.fit_transform(x_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresiones lineales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase base de la que heredan las que vayamos implementando\n",
    "#-----------------------------------------------------------\n",
    "# Es conveniente tener una clase base de la que vayan heredando las demás. Siempre habrá un método fit\n",
    "# y un método predict. Pero en esta clase base puede haber definiciones de atributos comunes a todas\n",
    "#===========================================================\n",
    "class BaseModel(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        return NotImplemented\n",
    "\n",
    "    def predict(self, X):\n",
    "        return NotImplemented\n",
    "\n",
    "\n",
    "class ConstantModel(BaseModel):\n",
    "    # El modelo constante solo saca la media de los datos y devuelve ese valor\n",
    "    # Es útil para comparar. Ningún modelo debería ser peor que este.\n",
    "    #-------------------------------------------------------------------------\n",
    "    def fit(self, X, Y):\n",
    "        W = Y.mean()\n",
    "        self.model = W\n",
    "\n",
    "    def predict(self, X):\n",
    "        # La \"predicción\" consiste en devolver la media para todos los valores\n",
    "        return np.ones(len(X)) * self.model\n",
    "\n",
    "# Modelo de la regresión lineal\n",
    "#==============================\n",
    "class LinearRegression(BaseModel):\n",
    "    # Este modelo de regresión lineal ajusta únicamente la pendiente, no contempla la ordenada al origen\n",
    "    def fit(self, X, y):\n",
    "        # Verificamos si X es un vector o una matriz\n",
    "        if len(X.shape) == 1:\n",
    "            # Esta es una manera de escribir la pseudo-inversa (X'.X)^(-1).X'.y\n",
    "            W = X.T.dot(y) / X.T.dot(X)\n",
    "        else:\n",
    "            # Y esta es la manera con matrices\n",
    "            W = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "        self.model = W\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X.dot(self.model)\n",
    "    \n",
    "# Modelo que incluye la ordenada al origen (b)\n",
    "# ============================================\n",
    "class LinearRegressionWithB(BaseModel):\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # En el caso de ajustar con ordenada al origen le agregamos la columna de b con unos\n",
    "        # (Le agrega la fila abajo y luego traspongo --> Vectores columna)\n",
    "        X_expanded = np.vstack((X, np.ones(len(X)))).T\n",
    "        W = np.linalg.inv(X_expanded.T.dot(X_expanded)).dot(X_expanded.T).dot(y)\n",
    "        self.model = W\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_expanded = np.vstack((X, np.ones(len(X)))).T\n",
    "        return X_expanded.dot(self.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradientes descendentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, lr=0.01, amt_epochs=100):\n",
    "    \"\"\"\n",
    "    lr: learning rate\n",
    "    amt_epochs: cantidad de iteraciones\n",
    "    \n",
    "    shapes: \n",
    "        X_t: nxm\n",
    "        Y_y: nx1\n",
    "        W: mx1\n",
    "    \"\"\"\n",
    "    n = X_train.shape[0]\n",
    "    m = X_train.shape[1]\n",
    "    # print('X.shape:{}x{}\\n'.format(n,m))\n",
    "        \n",
    "    # Inicializamos los pesos\n",
    "    W = np.random.randn(m).reshape(m,1)\n",
    "    print('W_inicial_{}'.format(W.reshape(-1)))\n",
    "    \n",
    "    for i in range(amt_epochs):\n",
    "        # Calculo la estimación\n",
    "        #y_hat=X_train*W\n",
    "        y_hat=np.matmul(X_train,W)\n",
    "        \n",
    "        # Calculo el error\n",
    "        error=y_train-y_hat\n",
    "        \n",
    "        # Calculo el gradiente\n",
    "        grad_sum = np.sum(error*X_train,axis=0)\n",
    "        grad_mul =-2/n*grad_sum  #1xm\n",
    "        gradient = np.transpose(grad_mul).reshape(-1,1) #mx1\n",
    "        \n",
    "        # Actualizo el valor\n",
    "        W = W - (lr*gradient)\n",
    "    \n",
    "    return W\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(X_train, y_train, lr=0.01, amt_epochs=100):\n",
    "    \"\"\"\n",
    "    lr: learning rate\n",
    "    amt_epochs: cantidad de iteraciones\n",
    "    \n",
    "    shapes: \n",
    "        X_t: nxm\n",
    "        Y_y: nx1\n",
    "        W: mx1\n",
    "    \"\"\"\n",
    "    n = X_train.shape[0]\n",
    "    m = X_train.shape[1]\n",
    "    # print('X.shape:{}x{}\\n'.format(n,m))\n",
    "        \n",
    "    # Inicializamos los pesos\n",
    "    W = np.random.randn(m).reshape(m,1)\n",
    "    print('W_inicial_{}'.format(W.reshape(-1)))\n",
    "    \n",
    "    for i in range(amt_epochs):\n",
    "        idx=np.random.permutation(X_train.shape[0])\n",
    "        X_train = X_train[idx]\n",
    "        y_train = y_train[idx]\n",
    "        \n",
    "        for j in range(n):\n",
    "        \n",
    "            # Calculo la estimación\n",
    "            #y_hat=X_train*W\n",
    "            y_hat=np.matmul(X_train[j].reshape(1,-1),W)\n",
    "\n",
    "            # Calculo el error\n",
    "            error=y_train[j]-y_hat\n",
    "\n",
    "            # Calculo el gradiente\n",
    "            grad_sum = error*X_train[j]\n",
    "            grad_mul =-2/n*grad_sum  #1xm\n",
    "            gradient = np.transpose(grad_mul).reshape(-1,1) #mx1\n",
    "\n",
    "            # Actualizo el valor\n",
    "            W = W - (lr*gradient)\n",
    "    \n",
    "    return W\n",
    "\n",
    "def mini_batch_gradient_descent(X_train, y_train, lr=0.01, amt_epochs=100):\n",
    "    \"\"\"\n",
    "    shapes:\n",
    "        X_t = nxm\n",
    "        y_t = nx1\n",
    "        W = mx1\n",
    "    \"\"\"\n",
    "    b = 16\n",
    "    n = X_train.shape[0]\n",
    "    m = X_train.shape[1]\n",
    "\n",
    "    # initialize random weights\n",
    "    W = np.random.randn(m).reshape(m, 1)\n",
    "\n",
    "    for i in range(amt_epochs):\n",
    "        idx = np.random.permutation(X_train.shape[0])\n",
    "        X_train = X_train[idx]\n",
    "        y_train = y_train[idx]\n",
    "\n",
    "        batch_size = int(len(X_train) / b)\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            end = i + batch_size if i + batch_size <= len(X_train) else len(X_train)\n",
    "            batch_X = X_train[i: end]\n",
    "            batch_y = y_train[i: end]\n",
    "\n",
    "            prediction = np.matmul(batch_X, W)  # nx1\n",
    "            error = batch_y - prediction  # nx1\n",
    "\n",
    "            grad_sum = np.sum(error * batch_X, axis=0)\n",
    "            grad_mul = -2/n * grad_sum  # 1xm\n",
    "            gradient = np.transpose(grad_mul).reshape(-1, 1)  # mx1\n",
    "\n",
    "            W = W - (lr * gradient)\n",
    "\n",
    "    return W\n",
    "\n",
    "\n",
    "def mini_batch_logistic_regression(X_train, y_train, lr=0.01, amt_epochs=100):\n",
    "    \"\"\"\n",
    "    shapes:\n",
    "        X_t = nxm\n",
    "        y_t = nx1\n",
    "        W = mx1\n",
    "    \"\"\"\n",
    "    b = 16\n",
    "    n = X_train.shape[0]\n",
    "    m = X_train.shape[1]\n",
    "\n",
    "    # initialize random weights\n",
    "    W = np.random.randn(m).reshape(m, 1)\n",
    "\n",
    "    for i in range(amt_epochs):\n",
    "        idx = np.random.permutation(X_train.shape[0])\n",
    "        X_train = X_train[idx]\n",
    "        y_train = y_train[idx]\n",
    "\n",
    "        batch_size = int( len(X_train) / b)\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            end = i + batch_size if i + batch_size <= len(X_train) else len(X_train)\n",
    "            batch_X = X_train[i: end]\n",
    "            batch_y = y_train[i: end]\n",
    "            prediction = 1/(1+np.exp(-np.matmul(batch_X, W))) #Ojo que no es la predicción posta!!\n",
    "            error = batch_y - prediction  # nx1\n",
    "\n",
    "            grad_sum = np.sum(error * batch_X, axis=0)\n",
    "            grad_mul = -2/b * grad_sum  # 1xm\n",
    "            gradient = np.transpose(grad_mul).reshape(-1, 1)  # mx1\n",
    "\n",
    "            W = W - (lr * gradient)\n",
    "\n",
    "    return W\n",
    "\n",
    "def sin_fitting_example():\n",
    "    # y = sin(x)\n",
    "    amt_points = 36\n",
    "    x = np.linspace(0, 360, num=amt_points)\n",
    "    y = np.sin(x * np.pi / 180.)\n",
    "    noise = np.random.normal(0, .1, y.shape)\n",
    "    noisy_y = y + noise\n",
    "\n",
    "    X_train = x\n",
    "    y_train = noisy_y\n",
    "\n",
    "    regression = LinearRegression()\n",
    "\n",
    "    # linear\n",
    "    X_linear = np.vstack((X_train, np.ones(len(X_train)))).T\n",
    "    regression.fit(X_linear, y_train.reshape(-1, 1))\n",
    "    W_linear = regression.model\n",
    "    y_linear = W_linear[0]*x + W_linear[1]\n",
    "\n",
    "    # quadratic\n",
    "    X_quadratic = np.vstack((np.power(X_train, 2), X_train, np.ones(len(X_train)))).T\n",
    "    regression.fit(X_quadratic, y_train.reshape(-1, 1))\n",
    "    W_quadratic = regression.model\n",
    "    y_quadratic = W_quadratic[0] * np.power(x, 2) + W_quadratic[1] * x + W_quadratic[2]\n",
    "\n",
    "    # cubic\n",
    "    X_cubic = np.vstack((np.power(X_train, 3), np.power(X_train, 2), X_train, np.ones(len(X_train)))).T\n",
    "    regression.fit(X_cubic, y_train.reshape(-1, 1))\n",
    "    W_cubic = regression.model\n",
    "    y_cubic = W_cubic[0] * np.power(x, 3) + W_cubic[1] * np.power(x, 2) + W_cubic[2] * x + W_cubic[3]\n",
    "\n",
    "    # X10\n",
    "    X_10 = np.vstack((np.power(X_train, 10), np.power(X_train, 9), np.power(X_train, 8),\n",
    "                      np.power(X_train, 7), np.power(X_train, 6), np.power(X_train, 5),\n",
    "                      np.power(X_train, 4), np.power(X_train, 3), np.power(X_train, 2),\n",
    "                      X_train, np.ones(len(X_train)))).T\n",
    "    regression.fit(X_10, y_train.reshape(-1, 1))\n",
    "    W_10 = regression.model\n",
    "    y_10 = W_10[0] * np.power(x, 10) + W_10[1] * np.power(x, 9) + W_10[2] * np.power(x, 8) + \\\n",
    "           W_10[3] * np.power(x, 7) + W_10[4] * np.power(x, 6) + W_10[5] * np.power(x, 5) + \\\n",
    "           W_10[6] * np.power(x, 4) + W_10[7] * np.power(x, 3) + W_10[8] * np.power(x, 2) + \\\n",
    "           W_10[9] * x + W_10[10]\n",
    "\n",
    "    # PLOTS\n",
    "    plt.figure()\n",
    "    plt.subplot(1, 1, 1)\n",
    "    plt.gca().set_title('Sin(x) - Fitting curves')\n",
    "\n",
    "    # original\n",
    "    plt.plot(x, noisy_y, 'o')\n",
    "\n",
    "    # linear\n",
    "    plt.plot(x, y_linear, '-')\n",
    "\n",
    "    # quadratic\n",
    "    plt.plot(x, y_quadratic, '-')\n",
    "\n",
    "    # cubic\n",
    "    plt.plot(x, y_cubic, '-')\n",
    "\n",
    "    # 10 power\n",
    "    plt.plot(x, y_10, '-')\n",
    "\n",
    "    plt.legend(['noisy signal', 'linear', 'quadratic', 'cubic', '10th power'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_folds(X_train, y_train, k=5):\n",
    "    l_regression = LinearRegression()\n",
    "    error = MSE()\n",
    "\n",
    "    chunk_size = int(len(X_train) / k)\n",
    "    mse_list = []\n",
    "    for i in range(0, len(X_train), chunk_size):\n",
    "        end = i + chunk_size if i + chunk_size <= len(X_train) else len(X_train)\n",
    "        new_X_valid = X_train[i: end]\n",
    "        new_y_valid = y_train[i: end]\n",
    "        new_X_train = np.concatenate([X_train[: i], X_train[end:]])\n",
    "        new_y_train = np.concatenate([y_train[: i], y_train[end:]])\n",
    "\n",
    "        l_regression.fit(new_X_train, new_y_train)\n",
    "        prediction = l_regression.predict(new_X_valid)\n",
    "        mse_list.append(error(new_y_valid, prediction))\n",
    "\n",
    "    mean_MSE = np.mean(mse_list)\n",
    "\n",
    "    return mean_MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clases de métricas\n",
    "#===================\n",
    "\n",
    "# Clase madre\n",
    "class Metric(object):\n",
    "    def __call__(self, target, prediction):\n",
    "        return NotImplemented\n",
    "\n",
    "# Por ahora solo esta --> Error cuadrático medio\n",
    "class MSE(Metric):\n",
    "    def __init__(self):\n",
    "        Metric.__init__(self)\n",
    "        \n",
    "    def __call__(self, target, prediction):\n",
    "        n = target.size\n",
    "        return np.sum((target - prediction) ** 2) / n\n",
    "\n",
    "class Precision(Metric):\n",
    "    def __init__(self):\n",
    "        Metric.__init__(self)\n",
    "        \n",
    "    def __call__(self, truth, prediction):\n",
    "        \n",
    "        # Encontramos los True Positive\n",
    "        true_pos_mask = (prediction == 1) & (truth == 1)\n",
    "        true_pos = true_pos_mask.sum()\n",
    "        \n",
    "        # Encontramos los False Positive\n",
    "        false_pos_mask = (prediction == 1) & (truth == 0)\n",
    "        false_pos = false_pos_mask.sum()\n",
    "        \n",
    "        return true_pos / (true_pos + false_pos)\n",
    "\n",
    "class Recall (Metric):\n",
    "    def __init__(self):\n",
    "        Metric.__init__(self)\n",
    "        \n",
    "    def __call__(self, truth, prediction):\n",
    "        \n",
    "        # Encontramos los True Positive\n",
    "        true_pos_mask = (prediction == 1) & (truth == 1)\n",
    "        true_pos = true_pos_mask.sum()\n",
    "        \n",
    "        # Encontramos los False Negative\n",
    "        false_neg_mask = (prediction == 0) & (truth == 1)\n",
    "        false_neg = false_neg_mask.sum()\n",
    "        \n",
    "        return true_pos / (true_pos + false_neg)\n",
    "        \n",
    "class Accuracy (Metric):\n",
    "    def __init__(self):\n",
    "        Metric.__init__(self)\n",
    "        \n",
    "    def __call__(self, truth, prediction):\n",
    "        \n",
    "        # Encontramos los True Positive\n",
    "        true_pos_mask = (prediction == 1) & (truth == 1)\n",
    "        true_pos = true_pos_mask.sum()\n",
    "        \n",
    "        # Encontramos los False Positive\n",
    "        false_pos_mask = (prediction == 1) & (truth == 0)\n",
    "        false_pos = false_pos_mask.sum()\n",
    "        \n",
    "        # Encontramos los True Negative\n",
    "        true_neg_mask = (prediction == 0) & (truth == 0)\n",
    "        true_neg = true_neg_mask.sum() \n",
    "    \n",
    "        # Encontramos los False Negative\n",
    "        false_neg_mask = (prediction == 0) & (truth == 1)\n",
    "        false_neg = false_neg_mask.sum()\n",
    "    \n",
    "        return (true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examen Final - Intro a AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Punto 2\n",
    "\n",
    "- a) Obtener el dataset desde el siguiente link. La primera columna representa los datos de entrada y la segunda columna representa los datos de salida.\n",
    "\n",
    "- b) Levantar el dataset en un arreglo de Numpy. \n",
    "\n",
    "- c) Graficar el dataset de manera tal que sea posible visualizar la nube de puntos.\n",
    "\n",
    "- d) Partir el dataset en train (80%) y test (20%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# b) Levantamos en un arreglo Numpy los datos\n",
    "dataset = Data('../clase_8_dataset.csv')\n",
    "# c) Para mostrarlos todos hago un \"split mentiroso\" al 100%. Todos los datos quedan en Train\n",
    "X_train, X_test, y_train, y_test = dataset.split(1)\n",
    "# Graficamos...\n",
    "plt.scatter(X_train, y_train, s=10, label='Dataset')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# d) Ahora sí particionamos el dataset en train (80%) y test (20%)\n",
    "X_train, X_test, y_train, y_test = dataset.split(0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Punto 3\n",
    "\n",
    "Utilizar regresión polinómica para hacer “fit” sobre la nube de puntos del train. Para este ejercicio, se desea utilizar la fórmula cerrada de la optimización polinómica. El modelo es de la forma y = [Wn … W0] * [X^n    X^(n-1)    …    1]. \n",
    "\n",
    "- a) Para n = 1 (modelo lineal con ordenada al origen), hacer un fit del modelo utilizando K-FOLDS. Para K-FOLDS partir el train dataset en 5 partes iguales, utilizar 4/5 para entrenar y 1/5 para validar. Informar el mejor modelo obtenido y el criterio utilizado para elegir dicho modelo (dejar comentarios en el código).\n",
    "\n",
    "- b) Repetir el punto (a), para n = {2,3,4}. Computar el error de validación y test del mejor modelo para cada n.\n",
    "\n",
    "- c) Elegir el polinomio que hace mejor fit sobre la nube de puntos y explicar el criterio seleccionado (dejar comentarios en el código). \n",
    "\n",
    "- d) Graficar el polinomio obtenido y el dataset de test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error de ajuste n=1: 925.676671178207\n",
      "Error de ajuste n=2: 101.69352938450034\n",
      "Error de ajuste n=3: 8.739423453141244\n",
      "Error de ajuste n=4: 8.776601235461433\n"
     ]
    }
   ],
   "source": [
    "# a) K-folds para el caso lineal\n",
    "# n=1: Caso Lineal\n",
    "X_linear = np.vstack((X_train, np.ones(len(X_train)))).T\n",
    "mean_MSE = k_folds(X_linear, y_train.reshape(-1, 1), k=5)\n",
    "print(\"Error de ajuste n=1: {}\".format(mean_MSE))\n",
    "\n",
    "# b) K-folds para los demás órdenes\n",
    "# n=2: Caso cuadratico\n",
    "X_quadratic = np.vstack((np.power(X_train, 2), X_train, np.ones(len(X_train)))).T\n",
    "mean_MSE = k_folds(X_quadratic, y_train, k=5)\n",
    "print(\"Error de ajuste n=2: {}\".format(mean_MSE))\n",
    "\n",
    "# n=3: Caso cúbico\n",
    "X_cubic = np.vstack((np.power(X_train, 3), np.power(X_train, 2), X_train, np.ones(len(X_train)))).T\n",
    "mean_MSE = k_folds(X_cubic, y_train, k=5)\n",
    "print(\"Error de ajuste n=3: {}\".format(mean_MSE))\n",
    "\n",
    "# n=4: Caso poly 4\n",
    "X_4 = np.vstack((np.power(X_train, 4), np.power(X_train, 3), np.power(X_train, 2), X_train, np.ones(len(X_train)))).T\n",
    "mean_MSE = k_folds(X_4, y_train, k=5)\n",
    "print(\"Error de ajuste n=4: {}\".format(mean_MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Conclusión:: \n",
    "De estos resultados podemos ver que el error de ajuste para n=3 es el más pequeño (inclusive mejor que n=4). Tomamos entonces n=3 para seguir. Todo esto está basado en la técnica de K-folds que va partiendo el train dataset en las partes que se le indique (en nuestro caso k=5) y encontrando un promedio de los errores cuadráticos medios entrenando sobre 4/5 de las particiones y validando sobre 1/5, para todas las combinaciones.\n",
    "\n",
    "En resumen, como dijimos, tomamos n=3 para seguir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# d) Graficamos el polinomio y el dataset de test\n",
    "# n=3: Caso cúbico\n",
    "X_cubic = np.vstack((np.power(X_train, 3), np.power(X_train, 2), X_train, np.ones(len(X_train)))).T\n",
    "\n",
    "x = np.linspace(-400, 400, num=800)\n",
    "\n",
    "regression = LinearRegression()\n",
    "regression.fit(X_cubic, y_train.reshape(-1, 1))\n",
    "W_cubic = regression.model\n",
    "y_cubic = W_cubic[0] * np.power(x, 3) + W_cubic[1] * np.power(x, 2) + W_cubic[2] * x + W_cubic[3]\n",
    "\n",
    "# Graficamos\n",
    "plt.scatter(X_test, y_test, s=10, label='Test data')\n",
    "plt.plot(x, y_cubic, 'r-')\n",
    "plt.legend(['Ajuste (n=3)','Test data'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Punto 4: Para el mejor modelo seleccionado en (3c) (el mejor “n”), hacer la optimización utilizando Mini-Batch Gradient Descent (partir el train dataset en 4/5 para entrenar y 1/5 para validar).\n",
    "\n",
    "- a) Para cada epoch, calcular el error de train y el error de validation.\n",
    "- b) Graficar el error de train y el error de validación en función del número de epoch.\n",
    "- c) Comparar los resultados obtenidos para el modelo entrenado con Mini-Batch, contra el modelo obtenido en (3c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================\n",
    "# Código que tenía para pruebas\n",
    "#==============================\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     dataset = Data('../income.data.csv')\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = dataset.split(0.8)\n",
    "\n",
    "#     linear_regression = LinearRegression()\n",
    "#     linear_regression.fit(X_train, y_train)\n",
    "#     lr_y_hat = linear_regression.predict(X_test)\n",
    "\n",
    "#     linear_regression_b = LinearRegressionWithB()\n",
    "#     linear_regression_b.fit(X_train, y_train)\n",
    "#     lrb_y_hat = linear_regression_b.predict(X_test)\n",
    "\n",
    "#     constant_model = ConstantModel()\n",
    "#     constant_model.fit(X_train, y_train)\n",
    "#     ct_y_hat = constant_model.predict(X_test)\n",
    "\n",
    "#     mse = MSE()\n",
    "#     lr_mse = mse(y_test, lr_y_hat)\n",
    "#     lrb_mse = mse(y_test, lrb_y_hat)\n",
    "#     ct_mse = mse(y_test, ct_y_hat)\n",
    "\n",
    "#     x_plot = np.linspace(0, 10, 10)\n",
    "#     lr_y_plot = linear_regression.model * x_plot\n",
    "#     lrb_y_plot = linear_regression_b.model[0] * x_plot + linear_regression_b.model[1]\n",
    "\n",
    "#     # gradient descent\n",
    "#     print('\\n\\n\\nGRADIENT DESCENT VS LINEAR REGRESSION')\n",
    "#     lr_1 = 0.001\n",
    "#     amt_epochs_1 = 1000\n",
    "#     start = time.time()\n",
    "#     W_manual = gradient_descent(X_train.reshape(-1, 1), y_train.reshape(-1, 1), lr=lr_1, amt_epochs=amt_epochs_1)\n",
    "#     time_1 = time.time() - start\n",
    "#     W_real = linear_regression.model\n",
    "#     print('W_manual:  {}\\nW_real:    {}\\nManual time [s]: {}'.format(W_manual.reshape(-1), W_real, time_1))\n",
    "\n",
    "#     # gradient descent\n",
    "#     print('\\n\\n\\nGRADIENT DESCENT VS LINEAR REGRESSION WITH B')\n",
    "#     X_expanded = np.vstack((X_train, np.ones(len(X_train)))).T\n",
    "#     lr_2 = 0.001\n",
    "#     amt_epochs_2 = 100000\n",
    "#     start = time.time()\n",
    "#     W_manual = gradient_descent(X_expanded, y_train.reshape(-1, 1), lr=lr_2, amt_epochs=amt_epochs_2)\n",
    "#     time_2 = time.time() - start\n",
    "#     W_real = linear_regression_b.model\n",
    "#     print('W_manual:  {}\\nW_real:    {}\\nManual time [s]: {}'.\n",
    "#           format(W_manual.reshape(-1), W_real, time_2))\n",
    "\n",
    "#     # gradient descent\n",
    "#     print('\\n\\n\\nSTOCHASTIC GRADIENT DESCENT VS LINEAR REGRESSION WITH B')\n",
    "#     X_expanded = np.vstack((X_train, np.ones(len(X_train)))).T\n",
    "#     lr_3 = 0.05\n",
    "#     amt_epochs_3 = 1000\n",
    "#     start = time.time()\n",
    "#     W_manual = stochastic_gradient_descent(X_expanded, y_train.reshape(-1, 1), lr=lr_3, amt_epochs=amt_epochs_3)\n",
    "#     time_3 = time.time() - start\n",
    "#     W_real = linear_regression_b.model\n",
    "#     print('W_manual:  {}\\nW_real:    {}\\nManual time [s]: {}'.\n",
    "#           format(W_manual.reshape(-1), W_real, time_3))\n",
    "\n",
    "#     # gradient descent\n",
    "#     print('\\n\\n\\nMINI BATCH GRADIENT DESCENT VS LINEAR REGRESSION WITH B')\n",
    "#     X_expanded = np.vstack((X_train, np.ones(len(X_train)))).T\n",
    "#     lr_4 = 0.05\n",
    "#     amt_epochs_4 = 10000\n",
    "#     start = time.time()\n",
    "#     W_manual = mini_batch_gradient_descent(X_expanded, y_train.reshape(-1, 1), lr=lr_4, amt_epochs=amt_epochs_4)\n",
    "#     time_4 = time.time() - start\n",
    "#     W_real = linear_regression_b.model\n",
    "#     print('W_manual:  {}\\nW_real:    {}\\nManual time [s]: {}'.\n",
    "#           format(W_manual.reshape(-1), W_real, time_4))\n",
    "\n",
    "#     # PLOTS\n",
    "#     plt.figure()\n",
    "#     x_plot = np.linspace(1, 4, 4)\n",
    "#     legend = ['GD', 'GD(B)', 'S-GD(B)', 'MB-GD(B)']\n",
    "\n",
    "#     plt.subplot(1, 3, 1)\n",
    "#     plt.gca().set_title('Learning Rate')\n",
    "#     y_plot = [lr_1, lr_2, lr_3, lr_4]\n",
    "#     plt.plot(x_plot[0], y_plot[0], 'o', x_plot[1], y_plot[1], 'o', x_plot[2], y_plot[2], 'o',\n",
    "#              x_plot[3], y_plot[3], 'o')\n",
    "#     plt.legend(legend)\n",
    "#     for x, y in zip(x_plot, y_plot):\n",
    "#         plt.text(x, y, str(y))\n",
    "\n",
    "#     plt.subplot(1, 3, 2)\n",
    "#     plt.gca().set_title('Epochs')\n",
    "#     y_plot = [amt_epochs_1, amt_epochs_2, amt_epochs_3, amt_epochs_4]\n",
    "#     plt.plot(x_plot[0], y_plot[0], 'o', x_plot[1], y_plot[1], 'o', x_plot[2], y_plot[2], 'o',\n",
    "#              x_plot[3], y_plot[3], 'o')\n",
    "#     plt.legend(legend)\n",
    "#     for x, y in zip(x_plot, y_plot):\n",
    "#         plt.text(x, y, str(y))\n",
    "\n",
    "#     plt.subplot(1, 3, 3)\n",
    "#     plt.gca().set_title('Time')\n",
    "#     y_plot = [time_1, time_2, time_3, time_4]\n",
    "#     plt.plot(x_plot[0], y_plot[0], 'o', x_plot[1], y_plot[1], 'o', x_plot[2], y_plot[2], 'o',\n",
    "#              x_plot[3], y_plot[3], 'o')\n",
    "#     plt.legend(legend)\n",
    "#     for x, y in zip(x_plot, y_plot):\n",
    "#         plt.text(x, y, str(y))\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "#     # sin fitting example\n",
    "#     sin_fitting_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded\n",
      "Dataset split\n",
      "Training\n",
      "W: [  0.12245323   0.11537612 -13.68000208]\n",
      "Time [s]: 50.46697926521301\n"
     ]
    }
   ],
   "source": [
    "#==============================\n",
    "# Código que tenía para pruebas\n",
    "#==============================\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     dataset = Data('../clase_6_dataset.txt')\n",
    "#     print('Dataset loaded')\n",
    "#     X_train, X_test, y_train, y_test = dataset.split(1)\n",
    "#     print('Dataset split')\n",
    "\n",
    "#     # gradient descent (es importante que tenga la columna de unos)\n",
    "#     X_expanded = np.vstack((X_train['X1'], X_train['X2'], np.ones(len(X_train)))).T\n",
    "#     lr = 0.001\n",
    "#     amt_epochs = 50000\n",
    "#     print('Training')\n",
    "#     start = time.time()\n",
    "#     W = mini_batch_logistic_regression(X_expanded, y_train.reshape(-1, 1), lr=lr,amt_epochs=amt_epochs)\n",
    "#     time = time.time() - start\n",
    "#     print('W: {}\\nTime [s]: {}'.format(W.reshape(-1), time))\n",
    "\n",
    "#     # PLOTS\n",
    "#     # filter out the applicants that got admitted\n",
    "#     admitted = X_train[y_train == 1]\n",
    "#     # filter out the applicants that didn't get admission\n",
    "#     not_admitted = X_train[y_train == 0]\n",
    "\n",
    "#     # logistic regression\n",
    "#     x_regression = np.linspace(30, 100, 70) # De 30 a 100, 70 valores equiespaciados\n",
    "#     y_regression = (-x_regression*W[0] - W[2])/W[1]\n",
    "\n",
    "#     # plots\n",
    "#     plt.scatter(admitted['X1'], admitted['X2'], s=10, label='Admitted')\n",
    "#     plt.scatter(not_admitted['X1'], not_admitted['X2'], s=10, label='Not Admitted')\n",
    "#     plt.plot(x_regression, y_regression, '-', color='green', label='Regression')\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
