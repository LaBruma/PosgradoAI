{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicios Clase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo el módulo de numpy\n",
    "#-------------------------\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1 : Expansión de matrices (distancias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armo las matrices de prueba (una de 3x3 y otra de 2x3)\n",
    "# La idea es encontrar la distancia entre cada fila de X y cada fila de C\n",
    "#------------------------------------------------------------------------\n",
    "X=np.array([[1,2,3],[3,5,6],[7,8,9]], dtype=np.float64)\n",
    "C=np.array([[1,0,0],[0,1,1]], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 0. 0.]]\n",
      "\n",
      " [[0. 1. 1.]]]\n",
      "[[ 3.60555128  8.06225775 13.45362405]\n",
      " [ 2.44948974  7.07106781 12.72792206]]\n"
     ]
    }
   ],
   "source": [
    "# Expandimos (agregamos una dimensión)\n",
    "expanded_C=C[:,None]\n",
    "print(expanded_C)\n",
    "# Operamos con X por broadcasting\n",
    "distances=np.sqrt(np.sum((expanded_C-X)**2,axis=2))\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio2: Agrupar por distancias más pequeñas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Podemos pensar el parámetro axis como la dimensión que queremos comprimir (en este caso la filas, por eso va 0)\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "indexes = np.argmin(distances,axis=0)\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3: K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITERATIONS = 10\n",
    "\n",
    "# Función que realiza prepara los datos e itera llamando a k_means_loop (la que hace las cuentas)\n",
    "#------------------------------------------------------------------------------------------------\n",
    "def k_means(X, n_clusters):\n",
    "    centroids = np.eye(n_clusters, X.shape[1])\n",
    "    print(centroids)\n",
    "    for i in range(MAX_ITERATIONS):\n",
    "        print(\"Iteration # {}\".format(i))\n",
    "        centroids, cluster_ids = k_means_loop(X, centroids)\n",
    "        print(centroids)\n",
    "    return centroids, cluster_ids\n",
    "\n",
    "# En este loop que realiza los cálculos\n",
    "def k_means_loop(X, centroids):\n",
    "    # find labels for rows in X based in centroids values\n",
    "    expanded_centroids = centroids[:, None]\n",
    "    distances = np.sqrt(np.sum((expanded_centroids - X) ** 2, axis=2))  # Calculo las distancias (norma 2) de los centroides a cada vector de X\n",
    "    arg_min = np.argmin(distances, axis=0) # Devuelve los índices de los vectores con menor distancia a los centroides\n",
    "    \n",
    "    # Recalcular los centroides\n",
    "    for i in range(centroids.shape[0]):\n",
    "        centroids[i] = np.mean(X[arg_min == i, :], axis=0)\n",
    "    \n",
    "    return centroids, arg_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 3.]\n",
      " [3. 5. 6.]\n",
      " [7. 8. 9.]]\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "Iteration # 0\n",
      "[[       nan        nan        nan]\n",
      " [3.66666667 5.         6.        ]]\n",
      "Iteration # 1\n",
      "[[3.66666667 5.         6.        ]\n",
      " [       nan        nan        nan]]\n",
      "Iteration # 2\n",
      "[[       nan        nan        nan]\n",
      " [3.66666667 5.         6.        ]]\n",
      "Iteration # 3\n",
      "[[3.66666667 5.         6.        ]\n",
      " [       nan        nan        nan]]\n",
      "Iteration # 4\n",
      "[[       nan        nan        nan]\n",
      " [3.66666667 5.         6.        ]]\n",
      "Iteration # 5\n",
      "[[3.66666667 5.         6.        ]\n",
      " [       nan        nan        nan]]\n",
      "Iteration # 6\n",
      "[[       nan        nan        nan]\n",
      " [3.66666667 5.         6.        ]]\n",
      "Iteration # 7\n",
      "[[3.66666667 5.         6.        ]\n",
      " [       nan        nan        nan]]\n",
      "Iteration # 8\n",
      "[[       nan        nan        nan]\n",
      " [3.66666667 5.         6.        ]]\n",
      "Iteration # 9\n",
      "[[3.66666667 5.         6.        ]\n",
      " [       nan        nan        nan]]\n",
      "[[3.66666667 5.         6.        ]\n",
      " [       nan        nan        nan]]\n",
      "[0 0 0]\n"
     ]
    }
   ],
   "source": [
    "n_clusters = 2\n",
    "\n",
    "centroids,cluster_ids = k_means(X,n_clusters)\n",
    "\n",
    "print(centroids)\n",
    "print(cluster_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 4: Datasets sintéticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 2.07906012,  1.17819501,  0.00386533, -0.74700215],\n",
       "        [ 0.56394876,  2.13787848,  0.04382613,  0.48808554],\n",
       "        [ 1.30852153, -1.07009345,  0.21217501,  1.80782957],\n",
       "        [ 0.51252765, -0.21072129, -0.78948025, -0.15771401],\n",
       "        [ 0.13893462,  1.2861342 , -0.32596392,  1.45601399],\n",
       "        [ 0.46429145,  2.42248637, -0.386016  ,  2.85565452],\n",
       "        [ 0.38186149,  1.87640091, -1.22741235, -0.31111269],\n",
       "        [-0.37609807,  2.0982526 , -0.91535393,  0.51788962],\n",
       "        [ 0.86158504,  1.17142685,  1.0308557 , -0.70720813],\n",
       "        [ 0.59706476,  1.69801389, -0.35155595,  1.86663905]]),\n",
       " array([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids = np.array([[1,0,0,0],[0,1,0,0]],dtype=np.float32)\n",
    "\n",
    "def syntheticdataset(centroids,n_samples,inv_overlap):\n",
    "    # Agrego solape\n",
    "    centroids = centroids*inv_overlap\n",
    "    # Generamos más centroides\n",
    "    nube = np.repeat(centroids,n_samples/2,axis=0)\n",
    "    # Agrego ruido (de dimensión n_samples x 4)\n",
    "    nube = nube + np.random.normal(loc=0, scale=1, size=(n_samples,4))\n",
    "    \n",
    "    # Armo el array indicando a qué clúster pertenecen\n",
    "    cluster_ids = np.array([[0],[1]])\n",
    "    cluster_ids = np.repeat(cluster_ids, n_samples / 2, axis=0)\n",
    "    \n",
    "    return nube, cluster_ids\n",
    "    \n",
    "syntheticdataset(centroids,10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids = np.array([[1,0,0,0],[0,1,0,0]])\n",
    "np.repeat(centroids,4,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 5: Generación de una variable aleatoria\n",
    "\n",
    "Se busca generar una variable aleatoria con distribución exponencial. Se generará a partir de una variable aleatoria con distribución uniforme. Para hacerlo se utilizará el método de inversa generalizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una variable aleatoria exponencial\n",
    "def random_exp_variable(lambda_p,n):\n",
    "    U = np.random.uniform(low=0.0,high=1.0,size=n)\n",
    "    X=(-1/lambda_p)* np.log(1-U)  # Inversa generalizada de la función exponencial: lambda*e^(-lambda*x)\n",
    "    return(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 19 13 10 11  4  6  2  5  1  4  1  1  0  0  1  0  1  1  2]\n"
     ]
    }
   ],
   "source": [
    "# Lo constatamos\n",
    "X=random_exp_variable(1,100)\n",
    "hist,bins = np.histogram(X,20)\n",
    "print(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 6: Inversa Generalizada\n",
    "\n",
    "Variable aleatoria con fX(x)=3x^2 \\\\\n",
    "\n",
    "Entonces FX(x)= x^3  \\\\\n",
    "\n",
    "Por lo que la inversa: X=U^(1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  2  5  9  8 15 17 14 27]\n"
     ]
    }
   ],
   "source": [
    "n=100\n",
    "U = np.random.uniform(low=0.0,high=1.0,size=n)\n",
    "X= np.cbrt(U)\n",
    "hist,bins = np.histogram(X,10)\n",
    "print(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 7: Normalización de datos (z-score)\n",
    "\n",
    "Ingeniería de features I\n",
    "\n",
    "Normalización de datos:\n",
    "\n",
    "- 1: Restar media\n",
    "- 2: Dividir por desvío de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debe recibir los datos organizados en columnas\n",
    "def normalizar_datos(X):\n",
    "    X = X-np.mean(X,axis=0)/np.std(X,axis=0)\n",
    "    return(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = normalizar_datos(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 8: Reducción de datos (en base al desvío)\n",
    "\n",
    "PCA: Principal component Analisis (Reducción de la dimensión del dataset)\n",
    "\n",
    "X es de nxn\n",
    "\n",
    "- Paso 1: X-mean(X)\n",
    "- Paso 2: Matriz de covarianza de la traspuesta --> np.cov(X^t)\n",
    "- Paso 3: Calculamos autovalores y autovectores --> np.linalg.eig\n",
    "- Paso 4: Ordenamos autovectores según autovalores descendente\n",
    "- Paso 5: Proyectar el dataset sobre los autovalores más relevantes (d)\n",
    "          PCA = (X-mean(X)) V[:,:d]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos la PCA casera\n",
    "def PCA_casera(X):\n",
    "    # Paso 1\n",
    "    X_mean = X-np.mean(X,axis=0) # Unicamente centrados, por eso no uso la f anterior\n",
    "    # Paso 2\n",
    "    X_cov = np.cov(np.transpose(X_mean)) # Vale X_mean.T\n",
    "    # Paso 3\n",
    "    w, v = np.linalg.eig(X_cov)\n",
    "    # Paso 4\n",
    "    id_ord = np.argsort(w)[::-1] # Lo último entre corchetes lo invierte\n",
    "    mat_ord=v[id_ord]\n",
    "    # w=w[idx]\n",
    "    # v=v[:,idx]\n",
    "    # Paso 5\n",
    "    PCA = np.matmul(X_mean,v[:,:2])\n",
    "    return(w,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autovalores: [1.44918704e+07 2.66533903e-02 6.61536528e-01] \r\n",
      " Autovectores: [[ 1.45000829e-05 -8.36815592e-01  5.47484854e-01]\n",
      " [ 9.99999999e-01  4.04490748e-05  3.53403998e-05]\n",
      " [ 5.17186534e-05 -5.47484852e-01 -8.36815591e-01]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([ [0.4, 4800, 5.5], [0.7, 12104, 5.2], [1, 12500, 5.5], [1.5, 7002, 4.0] ])\n",
    "\n",
    "w,v = PCA_casera(x)\n",
    "\n",
    "print('Autovalores: {} \\r\\n Autovectores: {}'.format(w,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-d456a3af43b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Ahora comparamos contra la función embebida de PCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Ahora comparamos contra la función embebida de PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "x_std = StandardScaler(with_std=False).fit_transform(x)\n",
    "pca.fit_transform(x_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 9: Remover NaNs (versión hardcore)\n",
    "\n",
    "A lo guapo le sacamos filas y  columnas donde aparezcan NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.0000e-01 4.8000e+03        nan]\n",
      " [7.0000e-01 1.2104e+04 5.2000e+00]\n",
      " [       nan 1.2500e+04 5.5000e+00]\n",
      " [1.5000e+00 7.0020e+03 4.0000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Armamos el dataset con NaNs\n",
    "dataset = np.array([ [0.4, 4800, np.nan], [0.7, 12104, 5.2], [np.nan, 12500, 5.5], [1.5, 7002, 4.0] ])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim = 0: Elimina filas con NaNs\n",
    "# dim = 1: Elimina columnas con NaNs\n",
    "def nans_hard_removal(dataset,dim):\n",
    "    nans_mask = np.isnan(dataset) # Armo una matriz lógica con NaNs\n",
    "    inv_dim = 1-dim  # Invierto la dimensión por cómo está definido lo que hace la función\n",
    "    nans_ids = np.any(nans_mask,axis=inv_dim) # Obtengo los índices por filas o columnas\n",
    "    if inv_dim == 0:\n",
    "        dataset = dataset[:,~nans_ids] # Indexo por el negado de nans_ids\n",
    "    else:\n",
    "        dataset = dataset[~nans_ids,:] # Indexo por el negado de nans_ids\n",
    "    return dataset    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.0000e-01 1.2104e+04 5.2000e+00]\n",
      " [1.5000e+00 7.0020e+03 4.0000e+00]]\n"
     ]
    }
   ],
   "source": [
    "clean_dataset = nans_hard_removal(dataset,0)\n",
    "\n",
    "print(clean_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 10: Remoción suave de NaNs (reemplaza por el promedo de la fila/columna) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim = 0: Reemplaza NaNs con promedio de filas\n",
    "# dim = 1: Reemplaza NaNs con promedio de columnas\n",
    "def nans_soft_removal(data_set,dim):\n",
    "    rows,cols = np.where(np.isnan(data_set)) # Verifico donde hay NaNs\n",
    "    if dim==0:\n",
    "        cols_ids=np.arange(dataset.shape[1])\n",
    "        id_col = 0\n",
    "        for row in rows:\n",
    "            col = cols[id_col]\n",
    "            data_set[row,col] = np.mean(data_set[row,cols_ids!=col])\n",
    "            id_col = id_col+1\n",
    "    else:\n",
    "        rows_ids=np.arange(dataset.shape[0])\n",
    "        id_row = 0\n",
    "        for col in cols:\n",
    "            row = rows[id_row]\n",
    "            data_set[row,col] = np.mean(data_set[rows_ids!=row,col])\n",
    "            id_row = id_row+1\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.00000000e-01 4.80000000e+03 4.90000000e+00]\n",
      " [7.00000000e-01 1.21040000e+04 5.20000000e+00]\n",
      " [8.66666667e-01 1.25000000e+04 5.50000000e+00]\n",
      " [1.50000000e+00 7.00200000e+03 4.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "dataset = np.array([ [0.4, 4800, np.nan], [0.7, 12104, 5.2], [np.nan, 12500, 5.5], [1.5, 7002, 4.0] ])\n",
    "\n",
    "clean_dataset = nans_soft_removal(dataset,1)\n",
    "print(clean_dataset)\n",
    "\n",
    "# DUDA: Esto pisa el dataset. Tendría que hacer una copìa interna en la función para que no pase no?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 11: Training, Validation, Testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_datasets (orig_ds):\n",
    "    n = orig_ds.shape[0]\n",
    "    random_ids = np.random.permutation(n)\n",
    "    idx_70 = np.uint(n*0.7)\n",
    "    training_ds = orig_ds[random_ids[0:idx_70]]\n",
    "    idx_90 = np.uint(n*0.9)\n",
    "    validation_ds = orig_ds[random_ids[idx_70:idx_90]]\n",
    "    testing_ds = orig_ds[random_ids[idx_90:n]]\n",
    "    \n",
    "    return (training_ds,validation_ds,testing_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  8  5]\n",
      " [ 7  2  2]\n",
      " [ 1  5  5]\n",
      " [ 5  0  4]\n",
      " [ 3  2  1]\n",
      " [ 1  2  3]\n",
      " [ 9  7  1]\n",
      " [ 3  2  1]\n",
      " [ 2  7  8]\n",
      " [33 12  1]]\n",
      "training:[[3 2 1]\n",
      " [7 2 2]\n",
      " [9 7 1]\n",
      " [3 2 1]\n",
      " [4 8 5]\n",
      " [2 7 8]\n",
      " [1 5 5]] \n",
      " validation:[[33 12  1]\n",
      " [ 5  0  4]] \n",
      " testing:[[1 2 3]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([ [4, 8, 5], [7, 2, 2], [1, 5, 5], [5, 0, 4], [3, 2, 1], [1, 2, 3], [9, 7, 1], [3, 2, 1], [2, 7, 8], [33, 12, 1] ])\n",
    "print(x)\n",
    "\n",
    "training_ds, validation_ds, testing_ds = split_datasets(x)\n",
    "print('training:{} \\n validation:{} \\n testing:{}'.format(training_ds,validation_ds,testing_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 12: Integrador Clases 1 y 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
