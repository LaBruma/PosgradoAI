{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones matriciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo el módulo de numpy\n",
    "#-------------------------\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armo mi matriz de prueba\n",
    "#-------------------------\n",
    "mat=np.array([[1,2],[3,4],[0,1]], dtype=np.float64)\n",
    "#print(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1: Definición de normas\n",
    "\n",
    "Trabajando siempre en modo vectorizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defino las normas\n",
    "#------------------\n",
    "def norm_l0(m):\n",
    "    mask = m > 0\n",
    "    return np.sum(mask,axis=1) # Cuenta de valores mayores a cero\n",
    "\n",
    "def norm_l1(m):\n",
    "    abs_m=np.abs(m)\n",
    "    return np.sum(abs_m,axis=1) # Suma por filas (con axis=0 sería por columnas)\n",
    "\n",
    "def norm_l2(m):\n",
    "    sqr_m=np.square(m)\n",
    "    return np.sqrt(np.sum(sqr_m,axis=1))\n",
    "\n",
    "def norm_inf(m):\n",
    "    return np.max(m, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norma 0: [2 2 1]\n",
      "Norma 1: [3. 7. 1.]\n",
      "Norma 2: [2.23606798 5.         1.        ]\n",
      "Norma infinito: [2. 4. 1.]\n"
     ]
    }
   ],
   "source": [
    "norm0=norm_l0(mat)\n",
    "print('Norma 0: {}'.format(norm0))\n",
    "\n",
    "norm1=norm_l1(mat)\n",
    "print('Norma 1: {}'.format(norm1))\n",
    "\n",
    "norm2=norm_l2(mat)\n",
    "print('Norma 2: {}'.format(norm2))\n",
    "\n",
    "norminf=norm_inf(mat)\n",
    "print('Norma infinito: {}'.format(norminf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Definimos los tests de prueba\n",
    "\n",
    "Definición de la clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest import TestCase\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "#from clase_1.norm import norm_l0\n",
    "\n",
    "class NormTestCase(TestCase):\n",
    "\n",
    "    def test_norm_l0(self):\n",
    "        a = np.array([[1,2,3,4],[5,6,7,8]])\n",
    "        expected = np.array([4, 4])\n",
    "        result = norm_l0(a)\n",
    "        # Estas dos que siguen se utilizan más que nada cuando se verifica por arrays de un solo elemento\n",
    "        #self.assertTrue(expected, result)\n",
    "        #self.assertEqual(expected, result)\n",
    "        # En caso contrario (como este) utilizamos la función de testing de Numpy\n",
    "        np.testing.assert_equal(expected, result)\n",
    "        \n",
    "        a = np.array([[1, 0, 0, 4], [5, 6, 0, 8]])\n",
    "        expected = np.array([2, 3])\n",
    "        result = norm_l0(a)\n",
    "        np.testing.assert_equal(expected, result)\n",
    "        \n",
    "    def test_norm_l1(self):\n",
    "        a = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "        expected = np.array([10, 26])\n",
    "        result = norm_l1(a)\n",
    "        np.testing.assert_equal(expected, result)\n",
    "        \n",
    "        a = np.array([[-1, -2, -3, -4], [-5, -6, -7, -8]])\n",
    "        expected = np.array([10, 26])\n",
    "        result = norm_l1(a)\n",
    "        np.testing.assert_equal(expected, result)\n",
    "        \n",
    "    def test_norm_l2(self):\n",
    "        a = np.array([[1, 2], [3, 4]])\n",
    "        expected = np.array([math.sqrt(5), math.sqrt(25)])\n",
    "        result = norm_l2(a)\n",
    "        np.testing.assert_allclose(expected, result)\n",
    "        \n",
    "    def test_norm_inf(self):\n",
    "        a = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "        expected = np.array([4, 8])\n",
    "        result = norm_inf(a)\n",
    "        np.testing.assert_equal(expected, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un objeto de prueba y corremos los tests\n",
    "#=================================================\n",
    "\n",
    "test_obj = NormTestCase()\n",
    "\n",
    "test_obj.test_norm_l0()\n",
    "test_obj.test_norm_l1()\n",
    "test_obj.test_norm_l2()\n",
    "test_obj.test_norm_inf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2: Sorting\n",
    "\n",
    "Ordenar según la norma dos obtenida en el ejercicio anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n",
      "[[5 6 7 8]\n",
      " [1 2 3 4]]\n"
     ]
    }
   ],
   "source": [
    "# Sorting: Ordenamos segun la norma 2\n",
    "def ordenar_por_norma_l2(mat):\n",
    "    norm2=norm_l2(mat)\n",
    "    id_ord = np.argsort(norm2*-1) # ordena y devuelve los índices de los valores ordenados\n",
    "    # El *-1 invierte el orden dado que argsort devuelve de menor a mayor y queremos de mayor a menor\n",
    "    print(id_ord)\n",
    "    return mat[id_ord, :]\n",
    "\n",
    "a = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "\n",
    "mat_ord = ordenar_por_norma_l2(a)\n",
    "print(mat_ord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definimos los tests de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest import TestCase\n",
    "import numpy as np\n",
    "\n",
    "class SortTestCase(TestCase):\n",
    "    def test_sorting_by_norm_l2(self):\n",
    "        a = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "        sorted_a = ordenar_por_norma_l2(a)\n",
    "        np.testing.assert_equal(np.array([[5, 6, 7, 8], [1, 2, 3, 4]]), sorted_a)\n",
    "        \n",
    "        a = np.array([[1, 2, 3, 4], [10, 11, 12, 13], [5, 6, 7, 8]])\n",
    "        sorted_a = ordenar_por_norma_l2(a)\n",
    "        np.testing.assert_equal(np.array([[10, 11, 12, 13], [5, 6, 7, 8], [1, 2, 3, 4]]), sorted_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n",
      "[1 2 0]\n"
     ]
    }
   ],
   "source": [
    "# Creamos un objeto de prueba y corremos los tests\n",
    "#=================================================\n",
    "test_obj = SortTestCase()\n",
    "\n",
    "test_obj.test_sorting_by_norm_l2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3: Indexing\n",
    "\n",
    "Construir un índice para identificadores de usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DUDA1: No me parece igual lo que devuelve id2idx en este programa que en las diapositivas.\n",
    "# En las diapositivas parece que devuelve un vector del tamaño de idx máximo con las posiciones dentro del vector de ids para cada id\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Indexer(object):\n",
    "        \n",
    "    # Constructor\n",
    "    def __init__(self, ids):\n",
    "        # Obtenemos los elementos únicos de un array (por si se repiten?) ordenados\n",
    "        unique_ids = np.unique(ids)\n",
    "        # Generamos un vector con el tamaño del id de usuario más grande todo con valores -1\n",
    "        id2idx = np.ones(unique_ids.max() + 1, dtype=np.int64) * -1\n",
    "        # np.arange es equivalente a linspace en Matlab. Devuelve un vector de valores equiespaciados\n",
    "        id2idx[unique_ids] = np.arange(unique_ids.size)\n",
    "        # Ya creados los dos quedan como parámetros del objeto\n",
    "        self.id2idx = id2idx\n",
    "        self.idx2id = unique_ids\n",
    "        \n",
    "    # Obtener los índices a partir de las ocurrencias\n",
    "    def get_idxs(self, ids):\n",
    "        ids = self.id2idx[ids]\n",
    "        return ids, ids !=-1 # Devuelve los que no sean -1\n",
    "        \n",
    "    # Obtener las ocurrencias a partir de los índices\n",
    "    def get_ids(self, idxs):\n",
    "        return self.idx2id[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola:7\n",
      "[-1  0  1 -1 -1 -1 -1 -1  2 -1 -1 -1 -1  3  4  5  6]\n",
      "[ 1  2  8 13 14 15 16]\n",
      "[False]\n",
      "[ 1 16]\n"
     ]
    }
   ],
   "source": [
    "# Verificamos...\n",
    "#===============\n",
    "a = np.array([1, 13, 14, 15, 2, 16, 8], dtype=np.int64)\n",
    "\n",
    "indexador = Indexer(a)\n",
    "\n",
    "print(indexador.id2idx)\n",
    "print(indexador.idx2id)\n",
    "\n",
    "idxs, valid_idxs = indexador.get_idxs(np.array([11]))\n",
    "print(valid_idxs)\n",
    "\n",
    "print(indexador.get_ids(np.array([0,6])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definimos los test de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest import TestCase\n",
    "import numpy as np\n",
    "\n",
    "class IndexerTestCase(TestCase):\n",
    "    def test_indexer(self):\n",
    "        a = np.array([10, 13, 14, 15], dtype=np.int64)\n",
    "        indexer = Indexer(a)\n",
    "    \n",
    "        idxs, valid_idxs = indexer.get_idxs(np.array([13, 10]))\n",
    "        np.testing.assert_equal(np.array([True, True]), valid_idxs)\n",
    "        \n",
    "        ids = indexer.get_ids(idxs)\n",
    "        np.testing.assert_equal(np.array([13, 10]), ids)\n",
    "    \n",
    "        idxs, valid_idxs = indexer.get_idxs(np.array([13, 1, 2, 15]))\n",
    "        np.testing.assert_equal(np.array([True, False, False, True]), valid_idxs)\n",
    "        \n",
    "        ids = indexer.get_ids(idxs[valid_idxs])\n",
    "        np.testing.assert_equal(np.array([13, 15]), ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un objeto de prueba y corremos los tests\n",
    "#=================================================\n",
    "test_obj = IndexerTestCase()\n",
    "\n",
    "test_obj.test_indexer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 4: Precision, Recall & Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseMetric' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-5dc2dc1d3778>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mPrecision\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseMetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;31m# Encontramos los True Positive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mtrue_pos_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtruth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BaseMetric' is not defined"
     ]
    }
   ],
   "source": [
    "# DUDA1: No entiendo de qué está heredando... (BaseMetric)\n",
    "# DUDA2: Tampoco entiendo el sentido de usar el método __call__ acá\n",
    "\n",
    "class Precision(BaseMetric):\n",
    "    def __call__(self, prediction, truth):\n",
    "        \n",
    "        # Encontramos los True Positive\n",
    "        true_pos_mask = (prediction == 1) & (truth == 1)\n",
    "        true_pos = true_pos_mask.sum()\n",
    "        \n",
    "        # Encontramos los False Positive\n",
    "        false_pos_mask = (prediction == 1) & (truth == 0)\n",
    "        false_pos = false_pos_mask.sum()\n",
    "        \n",
    "        return true_pos / (true_pos + false_pos)\n",
    "        \n",
    "class Recall (BaseMetric):\n",
    "    def __call__(self, prediction, truth):\n",
    "        \n",
    "        # Encontramos los True Positive\n",
    "        true_pos_mask = (prediction == 1) & (truth == 1)\n",
    "        true_pos = true_pos_mask.sum()\n",
    "        \n",
    "        # Encontramos los False Negative\n",
    "        false_neg_mask = (prediction == 0) & (truth == 1)\n",
    "        false_neg = false_neg_mask.sum()\n",
    "        \n",
    "        return true_pos / (true_pos + false_neg)\n",
    "        \n",
    "class Accuracy (BaseMetric):\n",
    "    def __call__(self, prediction, truth):\n",
    "        \n",
    "        # Encontramos los True Positive\n",
    "        true_pos_mask = (prediction == 1) & (truth == 1)\n",
    "        true_pos = true_pos_mask.sum()\n",
    "        \n",
    "        # Encontramos los False Positive\n",
    "        false_pos_mask = (prediction == 1) & (truth == 0)\n",
    "        false_pos = false_pos_mask.sum()\n",
    "        \n",
    "        # Encontramos los True Negative\n",
    "        true_neg_mask = (prediction == 0) & (truth == 0)\n",
    "        true_neg = true_neg_mask.sum() \n",
    "    \n",
    "        # Encontramos los False Negative\n",
    "        false_pos_mask = (prediction == 0) & (truth == 1)\n",
    "        false_pos = false_neg_mask.sum()\n",
    "    \n",
    "        return (true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 5: Average query precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseMetric' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-587011fae30f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Menos mal que esto ya estaba hecho... =) Igual se entiende el concepto. Es como las métricas anteriores pero para queries.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mQueryMeanPrecision\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseMetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_rank\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruth_relevance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BaseMetric' is not defined"
     ]
    }
   ],
   "source": [
    "# Menos mal que esto ya estaba hecho... =) Igual se entiende el concepto. Es como las métricas anteriores pero para queries.\n",
    "# Sigue heredando de BaseMetric....que no se qué es.\n",
    "\n",
    "# DUDA: En la métrica final no faltaría sumar todo el vector y dividir por len(true_relevance_count_by_query)?\n",
    "\n",
    "class QueryMeanPrecision(BaseMetric):\n",
    "    def __call__(self, predicted_rank, truth_relevance, query_ids):\n",
    "\n",
    "        # Obtenemos la cuenta de consultas con al menos un documento relevante\n",
    "        true_relevance_mask = (truth_relevance == 1)       # Obtengo la máscara de queries que fueron relevantes\n",
    "        filtered_query_id = query_ids[true_relevance_mask] # Obtengo los Ids de las queries que fueron relevantes\n",
    "        filtered_true_relevance_count = np.bincount(filtered_query_id) # en las queries con relevance, cuento\n",
    "        \n",
    "        # Completamos la cuenta de consultas con ceros en las consultas sin documentos relevantes\n",
    "        unique_query_ids = np.unique(query_ids)            # Me quedo con ids únicos y ordenados\n",
    "        non_zero_count_idxs = np.where(filtered_true_relevance_count > 0)  # Me fijo los Ids que tuvieron algún query relevante\n",
    "        true_relevance_count = np.zeros(unique_query_ids.max() + 1)\n",
    "        true_relevance_count[non_zero_count_idxs] = filtered_true_relevance_count[non_zero_count_idxs]\n",
    "        \n",
    "        # Obtenemos la cuenta solo para las consultas existentes\n",
    "        true_relevance_count_by_query = true_relevance_count[unique_query_ids]\n",
    "        \n",
    "        # Obtenemos la cuenta para los documentos adquiridos (traídos, fetched)\n",
    "        fetched_documents_count = np.bincount(query_ids)[unique_query_ids]\n",
    "        \n",
    "        # Computamos la métrica\n",
    "        precision_by_query = true_relevance_count_by_query / fetched_documents_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 6: Average query precision at K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DUDA: Siguiendo la métrica del ejercicio anterior creo que habría que simplemente dividir por k...no?\n",
    "\n",
    "class QueryMeanPrecisionAtK(BaseMetric):\n",
    "    def __call__(self, predicted_rank, truth_relevance, query_ids, k):\n",
    "\n",
    "        # Obtenemos la cuenta de consultas con al menos un documento relevante\n",
    "        true_relevance_mask = (truth_relevance == 1)       # Obtengo la máscara de queries que fueron relevantes\n",
    "        filtered_query_id = query_ids[true_relevance_mask] # Obtengo los Ids de las queries que fueron relevantes\n",
    "        filtered_true_relevance_count = np.bincount(filtered_query_id) # en las queries con relevance, cuento\n",
    "        \n",
    "        # Completamos la cuenta de consultas con ceros en las consultas sin documentos relevantes\n",
    "        unique_query_ids = np.unique(query_ids)            # Me quedo con ids únicos y ordenados\n",
    "        non_zero_count_idxs = np.where(filtered_true_relevance_count > 0)  # Me fijo los Ids que tuvieron algún query relevante\n",
    "        true_relevance_count = np.zeros(unique_query_ids.max() + 1)\n",
    "        true_relevance_count[non_zero_count_idxs] = filtered_true_relevance_count[non_zero_count_idxs]\n",
    "        \n",
    "        # Obtenemos la cuenta solo para las consultas existentes\n",
    "        true_relevance_count_by_query = true_relevance_count[unique_query_ids]\n",
    "        \n",
    "        # Obtenemos la cuenta para los documentos adquiridos (traídos, fetched)\n",
    "        fetched_documents_count = np.bincount(query_ids)[unique_query_ids]\n",
    "        \n",
    "        # Computamos la métrica\n",
    "        precision_by_query = true_relevance_count_by_query / k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 7: Computar todas las métricas con __call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-49f72228fdd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mCalcularMetricas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Precision\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Recall\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Accuracy\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Acá uso *args en lugar de **kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# Acá claramente estoy interpretando algo mal. Se que el método call permite llamar como si fuera función (es decir simplemente pasando)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-49f72228fdd9>\u001b[0m in \u001b[0;36mCalcularMetricas\u001b[1;34m(prediction, thruth, *args)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mmetrica\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Esto no es válido...pero no se me ocurre cómo hacer. Tendría que intanciar un objeto en función del key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;31m# Y si logro lo anterior entonces podría guardar los datos en un diccionario\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mmetricas\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmetrica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mthruth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "# Las métricas anteriores ya están definidas con __call__\n",
    "\n",
    "def CalcularMetricas(prediction,thruth,*args):\n",
    "    # Lo pienso para las métricas del ejercicio 4\n",
    "    metricas={}\n",
    "    idx=0\n",
    "    for key in args:\n",
    "        metrica = key() # Esto no es válido...pero no se me ocurre cómo hacer. Tendría que intanciar un objeto en función del key\n",
    "        # Y si logro lo anterior entonces podría guardar los datos en un diccionario\n",
    "        metricas [idx] = {key:metrica(prediction,thruth)}\n",
    "    return metricas\n",
    "\n",
    "truth = np.array([1,1,0,1,1,1,0,0,0,1])\n",
    "prediction = np.array([1,1,1,1,0,0,1,1,0,0])\n",
    "\n",
    "CalcularMetricas(\"Precision\",\"Recall\",\"Accuracy\") # Acá uso *args en lugar de **kwargs\n",
    "\n",
    "# Acá claramente estoy interpretando algo mal. Se que el método call permite llamar como si fuera función (es decir \n",
    "# simplemente pasando los argumentos como función). Usar **kwargs (en lugar de *args como puse acá) tiene que ver con llamar\n",
    "# a una función con número de parámetros indefinido, pero además pasándole valor y nombre (keyword), lo cual es útil para llamar\n",
    "# a funciones con argumentos variables. Pero no logro ver cómo juntar esto para las clases predefinidas..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 8: Transformar un dataset a numpy estructurado con singleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este hago agua mal....no se ni por dónde empezar..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
