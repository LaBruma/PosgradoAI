{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "corpus=['un hombre que no se alimenta de sus sueños envejece pronto','solamente aquel que construye el futuro tiene derecho a juzgar el pasado','los alimentos naturales son aquellos de origen vegetal o animal que cumplen la única condición requerida para no ser considerados procesados','volver al futuro , la curiosa historia de eric stoltz , el primer marty mcfly , a quien nadie soportaba y lo echaron del rodaje']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1: Implementar One-hot-encoding sobre el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(corpus):\n",
    "    corpus_words = []\n",
    "    # Primero \"armo\" el diccionario de todas las palabras\n",
    "    #----------------------------------------------------\n",
    "    for idx in range(len(corpus)):\n",
    "        # Armo la lista de tooooodas las palabras que aparecen en el corpus\n",
    "        corpus_words.extend(corpus[idx].split())\n",
    "        # Ahora encuentro los valores únicos\n",
    "        unique_words, indices = np.unique(corpus_words, return_index=True)\n",
    "    print(unique_words)\n",
    "        \n",
    "    # Después armo la matriz por oración\n",
    "    #-----------------------------------\n",
    "    for idx in range(len(corpus)):\n",
    "        # Vemos oración a oración si aparecen las palabras en el corpus\n",
    "        is_in = np.asarray([i in corpus[idx].split() for i in unique_words],dtype=np.int)\n",
    "        if 'ohe_mtx' in locals():\n",
    "            # Si ya existe la variable (tiene info) sigo agregando filas\n",
    "            ohe_mtx = np.concatenate((ohe_mtx, np.reshape(is_in, (1, -1))), axis=0)\n",
    "        else:\n",
    "            # Si no existe la variable (primera pasada) asigno la primer fila\n",
    "            ohe_mtx = np.reshape(is_in, (1, -1))\n",
    "        \n",
    "    return ohe_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',' 'a' 'al' 'alimenta' 'alimentos' 'animal' 'aquel' 'aquellos'\n",
      " 'condición' 'considerados' 'construye' 'cumplen' 'curiosa' 'de' 'del'\n",
      " 'derecho' 'echaron' 'el' 'envejece' 'eric' 'futuro' 'historia' 'hombre'\n",
      " 'juzgar' 'la' 'lo' 'los' 'marty' 'mcfly' 'nadie' 'naturales' 'no' 'o'\n",
      " 'origen' 'para' 'pasado' 'primer' 'procesados' 'pronto' 'que' 'quien'\n",
      " 'requerida' 'rodaje' 'se' 'ser' 'solamente' 'son' 'soportaba' 'stoltz'\n",
      " 'sueños' 'sus' 'tiene' 'un' 'vegetal' 'volver' 'y' 'única']\n",
      "\n",
      "\n",
      "[[0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "  0 0 1 1 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 1 1 1 0\n",
      "  0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1]\n",
      " [1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0\n",
      "  1 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0]]\n",
      "\n",
      "\n",
      "(4, 57)\n"
     ]
    }
   ],
   "source": [
    "ohe_mtx = one_hot_encoding(corpus)\n",
    "print('\\n')\n",
    "print(ohe_mtx)\n",
    "print('\\n')\n",
    "print(ohe_mtx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2: Implementar vectores de frecuencia sobre el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_vectors(corpus):\n",
    "    corpus_words = []\n",
    "    # Primero \"armo\" el diccionario de todas las palabras\n",
    "    #----------------------------------------------------\n",
    "    for idx in range(len(corpus)):\n",
    "        # Armo la lista de tooooodas las palabras que aparecen en el corpus\n",
    "        corpus_words.extend(corpus[idx].split())\n",
    "        # Ahora encuentro los valores únicos\n",
    "        unique_words, indices = np.unique(corpus_words, return_index=True)\n",
    "    \n",
    "    # Después armo la matriz por oración\n",
    "    #-----------------------------------\n",
    "    len_corpus = len(corpus)\n",
    "    len_unique_words = len(unique_words)\n",
    "    # Pongo en cero la matriz de frecuencias\n",
    "    freq_mtrx = np.zeros((len_corpus, len_unique_words))\n",
    "    \n",
    "    for idx in range(len_corpus):\n",
    "        # Recupero la oración\n",
    "        sentence = corpus[idx].split()\n",
    "        # Voy recorriendo el diccionario..\n",
    "        for idy in range(len_unique_words):\n",
    "            # Vemos oración a oración la cantidad de veces que aparece cada palabra en el corpus\n",
    "            freq_mtrx[idx,idy] = sentence.count(unique_words[idy])\n",
    "    return freq_mtrx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 0. 0. 1. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
      " [3. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0.\n",
      "  1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1.\n",
      "  1. 0. 0. 0. 0. 0. 1. 1. 0.]]\n",
      "\n",
      "\n",
      "(4, 57)\n"
     ]
    }
   ],
   "source": [
    "freq_mtrx = freq_vectors(corpus)\n",
    "print(freq_mtrx)\n",
    "print('\\n')\n",
    "print(freq_mtrx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3: Implementación TFIDF sobre el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(corpus):\n",
    "    corpus_words = []\n",
    "    # Primero \"armo\" el diccionario de todas las palabras\n",
    "    #----------------------------------------------------\n",
    "    for idx in range(len(corpus)):\n",
    "        # Armo la lista de tooooodas las palabras que aparecen en el corpus\n",
    "        corpus_words.extend(corpus[idx].split())\n",
    "        # Ahora encuentro los valores únicos\n",
    "        unique_words, indices = np.unique(corpus_words, return_index=True)\n",
    "    \n",
    "    # Después armo la matriz por oración\n",
    "    #-----------------------------------\n",
    "    N = len(corpus)\n",
    "    len_unique_words = len(unique_words)\n",
    "    # Pongo en cero la matriz de frecuencias (que básicamente...es el TF!!)\n",
    "    TF = np.zeros((N, len_unique_words))\n",
    "    \n",
    "    # Entonces calculamos el TF (lo mismo que los vectores de frecuencia)\n",
    "    #====================================================================\n",
    "    for idx in range(N):\n",
    "        # Recupero la oración\n",
    "        sentence = corpus[idx].split()\n",
    "        # Voy recorriendo el diccionario..\n",
    "        for idy in range(len_unique_words):\n",
    "            # Vemos oración a oración la cantidad de veces que aparece cada palabra en el corpus\n",
    "            TF[idx,idy] = sentence.count(unique_words[idy])\n",
    "    \n",
    "    # Ahora vamos por el IDF, que es cuántas veces aparece cada palabra en los documentos.\n",
    "    # Podemos sacarla con un truquete del TF\n",
    "    #=======================================\n",
    "    # Esto que vamos a hacer ahora es básicamente el one-hot-encoding\n",
    "    aux_IDF = np.asarray(TF >= 1, dtype=np.int)\n",
    "    # Ahora calculamos el IDF haciendo la suma sobre el one-hot-encoding\n",
    "    IDF = np.log(N/np.sum(aux_IDF,axis=0))\n",
    "    \n",
    "    # Finalmente calculamos el TFIDF\n",
    "    #===============================\n",
    "    TFIDF = TF*IDF\n",
    "    \n",
    "    return TFIDF\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         1.38629436 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.28768207 0.         0.         0.         0.\n",
      "  1.38629436 0.         0.         0.         1.38629436 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.69314718 0.         0.         0.         0.\n",
      "  0.         0.         1.38629436 0.28768207 0.         0.\n",
      "  0.         1.38629436 0.         0.         0.         0.\n",
      "  0.         1.38629436 1.38629436 0.         1.38629436 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.69314718 0.         0.         0.         0.\n",
      "  1.38629436 0.         0.         0.         1.38629436 0.\n",
      "  0.         0.         0.         1.38629436 0.         1.38629436\n",
      "  0.         0.         0.69314718 0.         0.         1.38629436\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         1.38629436\n",
      "  0.         0.         0.         0.28768207 0.         0.\n",
      "  0.         0.         0.         1.38629436 0.         0.\n",
      "  0.         0.         0.         1.38629436 0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         1.38629436 1.38629436\n",
      "  0.         1.38629436 1.38629436 1.38629436 0.         1.38629436\n",
      "  0.         0.28768207 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.69314718 0.         1.38629436 0.         0.         0.\n",
      "  1.38629436 0.69314718 1.38629436 1.38629436 1.38629436 0.\n",
      "  0.         1.38629436 0.         0.28768207 0.         1.38629436\n",
      "  0.         0.         1.38629436 0.         1.38629436 0.\n",
      "  0.         0.         0.         0.         0.         1.38629436\n",
      "  0.         0.         1.38629436]\n",
      " [4.15888308 0.69314718 1.38629436 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.38629436 0.28768207 1.38629436 0.         1.38629436 0.69314718\n",
      "  0.         1.38629436 0.69314718 1.38629436 0.         0.\n",
      "  0.69314718 1.38629436 0.         1.38629436 1.38629436 1.38629436\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.38629436 0.         0.         0.         1.38629436 0.\n",
      "  1.38629436 0.         0.         0.         0.         1.38629436\n",
      "  1.38629436 0.         0.         0.         0.         0.\n",
      "  1.38629436 1.38629436 0.        ]]\n",
      "(4, 57)\n"
     ]
    }
   ],
   "source": [
    "TFIDF = tfidf(corpus)\n",
    "print(TFIDF)\n",
    "print(TFIDF.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 4: Función de similaridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similars(corpus_matrix, array_pos = 1):\n",
    "    test_doc = corpus_matrix[array_pos,:]\n",
    "    docs_norm = np.sum(np.abs(corpus_matrix)**2,axis=-1)**(1./2)\n",
    "    test_doc_norm = np.sum(np.abs(test_doc)**2)**(1./2)\n",
    "    similars = np.argsort(np.dot(corpus_matrix,test_doc)/(docs_norm * test_doc_norm))[::-1]\n",
    "    \n",
    "    return similars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 0, 2], dtype=int64)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_similars(TFIDF,1)\n",
    "\n",
    "# Anda a la pelusa!. Por ejemplo el doc 1 habla del futuro al igual que el 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 5: TFIDF con Matrices escasas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def tfidf(corpus):\n",
    "    corpus_words = []\n",
    "    # Primero \"armo\" el diccionario de todas las palabras\n",
    "    #----------------------------------------------------\n",
    "    for idx in range(len(corpus)):\n",
    "        # Armo la lista de tooooodas las palabras que aparecen en el corpus\n",
    "        corpus_words.extend(corpus[idx].split())\n",
    "        # Ahora encuentro los valores únicos\n",
    "        unique_words, indices = np.unique(corpus_words, return_index=True)\n",
    "    \n",
    "    # Después armo la matriz por oración\n",
    "    #-----------------------------------\n",
    "    N = len(corpus)\n",
    "    len_unique_words = len(unique_words)\n",
    "    # Pongo en cero la matriz de frecuencias (que básicamente...es el TF!!)\n",
    "    TF = np.zeros((N, len_unique_words))\n",
    "    \n",
    "    # Entonces calculamos el TF (lo mismo que los vectores de frecuencia)\n",
    "    #====================================================================\n",
    "    for idx in range(N):\n",
    "        # Recupero la oración\n",
    "        sentence = corpus[idx].split()\n",
    "        # Voy recorriendo el diccionario..\n",
    "        for idy in range(len_unique_words):\n",
    "            # Vemos oración a oración la cantidad de veces que aparece cada palabra en el corpus\n",
    "            TF[idx,idy] = sentence.count(unique_words[idy])\n",
    "    \n",
    "    # Ahora vamos por el IDF, que es cuántas veces aparece cada palabra en los documentos.\n",
    "    # Podemos sacarla con un truquete del TF\n",
    "    #=======================================\n",
    "    # Esto que vamos a hacer ahora es básicamente el one-hot-encoding\n",
    "    aux_IDF = np.asarray(TF >= 1, dtype=np.int)\n",
    "    # Ahora calculamos el IDF haciendo la suma sobre el one-hot-encoding\n",
    "    IDF = np.log(N/np.sum(aux_IDF,axis=0))\n",
    "    \n",
    "    # Finalmente calculamos el TFIDF\n",
    "    #===============================\n",
    "    TFIDF = TF*IDF\n",
    "\n",
    "    # Acá es donde, sabiendo que tenemos una matriz esparsa, la transformamos en su representación\n",
    "    ind = np.where(TFIDF)\n",
    "    row = ind[0]\n",
    "    col = ind[1]\n",
    "    data = TFIDF[row,col]\n",
    "    \n",
    "    # Nueva matriz TFIDF comprimida esparsa\n",
    "    #======================================\n",
    "    TFIDF = csr_matrix((data,(row,col)), shape=(N,len_unique_words)).toarray()\n",
    "    \n",
    "    return TFIDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         1.38629436 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.28768207 0.         0.         0.         0.\n",
      "  1.38629436 0.         0.         0.         1.38629436 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.69314718 0.         0.         0.         0.\n",
      "  0.         0.         1.38629436 0.28768207 0.         0.\n",
      "  0.         1.38629436 0.         0.         0.         0.\n",
      "  0.         1.38629436 1.38629436 0.         1.38629436 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.69314718 0.         0.         0.         0.\n",
      "  1.38629436 0.         0.         0.         1.38629436 0.\n",
      "  0.         0.         0.         1.38629436 0.         1.38629436\n",
      "  0.         0.         0.69314718 0.         0.         1.38629436\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         1.38629436\n",
      "  0.         0.         0.         0.28768207 0.         0.\n",
      "  0.         0.         0.         1.38629436 0.         0.\n",
      "  0.         0.         0.         1.38629436 0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         1.38629436 1.38629436\n",
      "  0.         1.38629436 1.38629436 1.38629436 0.         1.38629436\n",
      "  0.         0.28768207 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.69314718 0.         1.38629436 0.         0.         0.\n",
      "  1.38629436 0.69314718 1.38629436 1.38629436 1.38629436 0.\n",
      "  0.         1.38629436 0.         0.28768207 0.         1.38629436\n",
      "  0.         0.         1.38629436 0.         1.38629436 0.\n",
      "  0.         0.         0.         0.         0.         1.38629436\n",
      "  0.         0.         1.38629436]\n",
      " [4.15888308 0.69314718 1.38629436 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.38629436 0.28768207 1.38629436 0.         1.38629436 0.69314718\n",
      "  0.         1.38629436 0.69314718 1.38629436 0.         0.\n",
      "  0.69314718 1.38629436 0.         1.38629436 1.38629436 1.38629436\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.38629436 0.         0.         0.         1.38629436 0.\n",
      "  1.38629436 0.         0.         0.         0.         1.38629436\n",
      "  1.38629436 0.         0.         0.         0.         0.\n",
      "  1.38629436 1.38629436 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "TFIDF = tfidf(corpus)\n",
    "print(TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
